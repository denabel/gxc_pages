[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "GESIS meets Copernicus (GxC)",
    "section": "",
    "text": "Welcome to the project website of “GESIS meets Copernicus” (GxC).\n\n\nFor many researchers in the social sciences, Earth observation (EO) data represents a black box. Social science researchers face many obstacles in applying and using these data, resulting from 1) a lack of technical expertise, 2) a lack of knowledge of data sources and how to access them, 3) unfamiliarity with complex data formats, such as high-resolution, longitudinal raster datacubes, and 4) a lack of expertise in integrating the data into existing social science datasets. GxC aims to close the gap by creating an automated interface to EO data and complementary resources for social science research.\n\n\n\n\n\n\n\nThe project’s core is creating an open-source tool to link time- and space-sensitive social science datasets with data from Earth observation programs. Detailed documentation and beginner-friendly tutorials complement the tool to showcase the capability of our project. The social science community is the main target group of our tool. At the same time, Earth system science researchers may similarly profit from integrated social science data. This project supports inter- and transdisciplinary research which is often made difficult because of technical, disciplinary, and organizational barriers. The project emphasizes research data management (RDM) workflows based on FAIR Data and Open Science principles. All code is written in the open-source software R and is made available on this website.\nBelow, you will find more information about navigation on this page."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "GESIS is one of the world’s leading infrastructure institutions for the social sciences and supports researchers with expertise and infrastructure services at all levels of their research projects. We help to ensure that socially relevant questions can be answered on the basis of reliable data, the latest scientific methods and research information. We help with study planning and data collection, provide high-quality research data, support with data processing and analysis and finally with archiving the data.\nhttps://www.gesis.org/en"
  },
  {
    "objectID": "index.html#welcome-to-my-package",
    "href": "index.html#welcome-to-my-package",
    "title": "GESIS meets Copernicus (GxC)",
    "section": "",
    "text": "Welcome to the official website for our R package! This package provides useful functionality to help with X, Y, and Z. You can use it for data analysis, visualization, and much more.\n\n\nExplore the documentation and tutorials to get started with our package, and feel free to contribute or report issues on our GitHub repository!"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "GESIS meets Copernicus (GxC)",
    "section": "",
    "text": "Welcome to the project website of “GESIS meets Copernicus” (GxC).\n\n\nFor many researchers in the social sciences, Earth observation (EO) data represents a black box. Social science researchers face many obstacles in applying and using these data, resulting from 1) a lack of technical expertise, 2) a lack of knowledge of data sources and how to access them, 3) unfamiliarity with complex data formats, such as high-resolution, longitudinal raster datacubes, and 4) a lack of expertise in integrating the data into existing social science datasets. GxC aims to close the gap by creating an automated interface to EO data and complementary resources for social science research.\n\n\n\n\n\n\n\nThe project’s core is creating an open-source tool to link time- and space-sensitive social science datasets with data from Earth observation programs. Detailed documentation and beginner-friendly tutorials complement the tool to showcase the capability of our project. The social science community is the main target group of our tool. At the same time, Earth system science researchers may similarly profit from integrated social science data. This project supports inter- and transdisciplinary research which is often made difficult because of technical, disciplinary, and organizational barriers. The project emphasizes research data management (RDM) workflows based on FAIR Data and Open Science principles. All code is written in the open-source software R and is made available on this website.\nBelow, you will find more information about navigation on this page."
  },
  {
    "objectID": "index.html#navigation",
    "href": "index.html#navigation",
    "title": "GESIS meets Copernicus (GxC)",
    "section": "Navigation",
    "text": "Navigation\nOn top, you find the following tabs:\n\nTutorials: Check out our tutorials and learning material\nSoftware: Documentation for the gxc-package\nCourses: Check out our upcoming and past courses and workshops\nArticles: Lists published and ongoing research based on GxC\nAbout: Information on GESIS, the research team and the project history\nMailing list: Sign up for the EOSS mailing list"
  },
  {
    "objectID": "package.html",
    "href": "package.html",
    "title": "Package",
    "section": "",
    "text": "The unique feature of the tool should be the possibility of carrying out both geographically and temporally high-resolution queries of data from Copernicus and other Earth observation data sources, which at the same time function efficiently on simple workstations albeit large amounts of data. Our tested workflow development has identified five major levers: indicator type, indicator intensity, focal time period, baseline time period, and spatial buffer. Flexibility on these five attributes should be maximized for users. The tool also offers the functionality to automatically derive spatio-temporal links with other georeferenced data (e.g., surveys, digital behavioral data).\n\n\n\n\n\n\n\nUsers should benefit from the core variables integrated into the interface for social research. Preparatory work is currently being carried out to select appropriate indicators from the corpus of indicators offered by data providers and conceptualize the data integration logic. Furthermore, in exchange with users and other stakeholders, we compile data products from EOD that are particularly relevant for social scientists. Examples include data on local air quality and pollutants, extreme weather events, or land use changes. The main data providers will be the Copernicus Monitoring Services on Climate Change, Atmosphere, and Land.\n\n\n\nMajor attributes for indicator specification. Source: Abel and Jünger 2024\n\n\nGESIS strongly supports the FAIR data principles and Open Data. The selection of R as the programming language for the tool supports open-source infrastructure development and shareability, as well as quality control via online repositories. Publishing the R scripts for data management and analysis ensures the reproducibility of all research steps."
  },
  {
    "objectID": "package.html#documentation",
    "href": "package.html#documentation",
    "title": "Package",
    "section": "Documentation",
    "text": "Documentation\ntba"
  },
  {
    "objectID": "articles.html",
    "href": "articles.html",
    "title": "Articles",
    "section": "",
    "text": "Abel, Dennis & Stefan Jünger. “Earth Observation Data in the Social Sciences - Opportunities and Limitations”\nAbel, Dennis, Manuel Linsenmeier & Stefan Jünger. “GeoCliP-GER - A georeferenced local climate policy database for Germany”\nAbel, Dennis & Stefan Jünger. “How flooding events shape subjective risk perceptions and climate change opinion”\nJünger, Stefan, Dennis Abel & Anne Stroppe. “The uncertain geographic context problem and the unknown effect of extreme weather on climate change attitudes”"
  },
  {
    "objectID": "articles.html#working-papers",
    "href": "articles.html#working-papers",
    "title": "Articles",
    "section": "",
    "text": "Abel, Dennis & Stefan Jünger. “Earth Observation Data in the Social Sciences - Opportunities and Limitations”\nAbel, Dennis, Manuel Linsenmeier & Stefan Jünger. “GeoCliP-GER - A georeferenced local climate policy database for Germany”\nAbel, Dennis & Stefan Jünger. “How flooding events shape subjective risk perceptions and climate change opinion”\nJünger, Stefan, Dennis Abel & Anne Stroppe. “The uncertain geographic context problem and the unknown effect of extreme weather on climate change attitudes”"
  },
  {
    "objectID": "articles.html#published",
    "href": "articles.html#published",
    "title": "Articles",
    "section": "Published",
    "text": "Published"
  },
  {
    "objectID": "about.html#project-history",
    "href": "about.html#project-history",
    "title": "About",
    "section": "Project history",
    "text": "Project history"
  },
  {
    "objectID": "about.html#the-research-team",
    "href": "about.html#the-research-team",
    "title": "About",
    "section": "The research team",
    "text": "The research team\n\nDr. Dennis Abel\nDr. Stefan Jünger"
  },
  {
    "objectID": "eod_intro.html",
    "href": "eod_intro.html",
    "title": "Introduction to Earth observation data",
    "section": "",
    "text": "Journey into EO data for social scientists (AI generated image).\n\n\nEarth observation (EO) data refers to all collected information about the Earth’s physical, chemical, and biological systems. Utilizing Earth observation data allows to study the Earth’s atmosphere, land cover, oceans and inland waters, as well as biological diversity and ecosystems. Applications of EO data in academia, industry, and policy-making are extensive. It is crucial for the operation of activities in environmental protection, energy management, urban planning, agriculture and fisheries, forestry, public health, transport and mobility, civil protection, or tourism.\n\n\nAdditional information on Earth system indicators\n\nThere are many crucial Earth system indicators. With respect to the Earth’s climate, for example, the Global Climate Observing System (GCOS) maps 55 Essential Climate Variables (ECVs). ECVs are seen as the empirical evidence base for the guidance of mitigation and adaptation measures, risk assessment and the understanding of climate services. EO data is essential for systematically measuring these variables. Check out their documentation of ECVs.\n\n\n\nECVs as proposed by GCOS. Source: GCOS 2024 (https://gcos.wmo.int/en/essential-climate-variables/)\n\n\n\nThere is often confusion about terms such as Earth observation data, geodata, remote sensing, or satellite data. Let’s disentangle these different concepts.\n\nEarth observation data 🌐: Information collected about the Earth’s system\nGeospatial data 🗺: Data that is georeferenced (includes information on the location)\nRemote sensing data 📡: Information that is acquired by sensors “from a distance”\nSatellite data 🛰: Information that is acquired by sensors via satellites\n\nAs you can see, these four terminologies address different aspects of the data. While “EO data” refers to the data content (the Earth system), “geospatial data” addresses the location, and “remote sensing” and “satellite data” address the way of obtaining the data.\nThe Venn diagram below visualizes these definitions and shows overlaps and gaps between these concepts. Except for satellite data, which can be seen as a subset of remote sensing data, all other concepts have overlaps and gaps with each other.\n\n\n\nEO Venn diagram. Source: Abel and Jünger 2024\n\n\nLet’s disentangle these overlaps a bit further:\n\nA: The area A captures all concepts discussed above - Georeferenced Earth observation data derived from satellite sensors. An example for this are nighttime lights 🌇. Night lights are highly informative for social scientists to measure population growth, electrification and light pollution, the expansion of urban areas, the impacts of natural events, and economic activity. An important satellite for nighttime lights is the NASA/NOAA Suomi National Polar-orbiting Partnership (Suomi NPP), which was launched in 2011. On board of Suomi NPP, the Visible Infrared Imaging Radiometer Suite (VIIRS) instrument observes nighttime lights with the day night band (DNB). Luckily for us, we do not have to handle this raw data. NASA’s Black Marble project offers various pre-processed products with high spatial and temporal resolution and temporal coverage since 2012. The data products are free and open access. R user benefit from the BlackMarbleR-package, which is a user-friendly interface to Black Marble data.\n\nB: Area B is similar to A except that this data is derived from remote sensing OTHER than satellite data. There are, in fact, other platforms for sensors, like aircrafts or drones. For example, data on gases in the atmosphere are collected that way. Often, airborne data has a smaller geographical coverage and is more case specific. Furthermore, it is much more often proprietary and costly. For example, commercial providers offer airborne measurements of GHG emissions on a very high spatial resolution (&lt;1m) to estimate point-source emissions of factories and power plants.\nC: C represents EO data which is georeferenced but NOT derived from remote sensing. There are several alternatives to EO data generation: Ground-based sensors (called “in-situ”) are widespread sources of EO data. Weather stations or local air quality sensors are common examples. Projects like Google’s StreetView can also be classified here. But C also captures a data source which is very familiar to social scientists: Field surveys and survey-based methods. Biodiversity monitoring and soil quality projects often employ these methods. This work is not necessarily performed by experts only. Citizen science projects have contributed immensely to mapping local biodiversity, air quality, or land use: For example, scientists work together with “laypeople” to map bird populations like the RSPB in the UK with the annual Big Garden Birdwatch. Check out the European Citizen Science Association website to learn more about this form of data generation.\nD and E: The two areas D and E deviate from the previous concepts because they are NOT EO data. D captures all geodata which is not derived from remote sensing and does not constitute EO data. This is primarily data which is human-centered - many examples of which are familiar to social scientists: Georeferenced variables on socio-demographics or economic indicators like Census data, electoral outcomes, or environmental attitudes and behavior. Examples for E, which represents data gathered based on remote sensing but is NOT classified as Earth observation, are space exploration and astronomy 🌌. Imaging of Mars’ surface is such an example. Future generations of social scientists might be concerned with interplanetary societal issues - (un)fortunately for us, we will focus on societies from planet Earth 🚀.\n\nAs you can see, the main areas when it comes to EO data are A, B, and C. As social scientists interested in working with EO data, D is similarly relevant: Often, we are interested in spatially linking our social indicators (D) with EO data. We therefore see these as two sides of the same coin. The geolocation represents the functional link between our social indicators and the Earth system context.\nThere are a few additional areas in the Venn diagram which we have not highlighted. These are the fields which do not overlap with the circle for “geodata”. Since we are focussing on geo-referenced data in this project, these parts will not be further considered in our tutorials.\nThe conceptual distinctions between the ways of obtaining data (in-situ, airborne, or satellite sensors) are important but often not clear cut in practice. Most of the time, we access processed (and/or simulated) data products 🎁, which are generated by integrating datastreams from various sensors to increase data quality. This is done to enhance accuracy, scope or resolution, helps to validate data, or to fill data gaps. We will explore a few common sources and indicators in the next chapter."
  },
  {
    "objectID": "eod_intro.html#what-is-earth-observation-data",
    "href": "eod_intro.html#what-is-earth-observation-data",
    "title": "Introduction to Earth observation data",
    "section": "",
    "text": "Journey into EO data for social scientists (AI generated image).\n\n\nEarth observation (EO) data refers to all collected information about the Earth’s physical, chemical, and biological systems. Utilizing Earth observation data allows to study the Earth’s atmosphere, land cover, oceans and inland waters, as well as biological diversity and ecosystems. Applications of EO data in academia, industry, and policy-making are extensive. It is crucial for the operation of activities in environmental protection, energy management, urban planning, agriculture and fisheries, forestry, public health, transport and mobility, civil protection, or tourism.\n\n\nAdditional information on Earth system indicators\n\nThere are many crucial Earth system indicators. With respect to the Earth’s climate, for example, the Global Climate Observing System (GCOS) maps 55 Essential Climate Variables (ECVs). ECVs are seen as the empirical evidence base for the guidance of mitigation and adaptation measures, risk assessment and the understanding of climate services. EO data is essential for systematically measuring these variables. Check out their documentation of ECVs.\n\n\n\nECVs as proposed by GCOS. Source: GCOS 2024 (https://gcos.wmo.int/en/essential-climate-variables/)\n\n\n\nThere is often confusion about terms such as Earth observation data, geodata, remote sensing, or satellite data. Let’s disentangle these different concepts.\n\nEarth observation data 🌐: Information collected about the Earth’s system\nGeospatial data 🗺: Data that is georeferenced (includes information on the location)\nRemote sensing data 📡: Information that is acquired by sensors “from a distance”\nSatellite data 🛰: Information that is acquired by sensors via satellites\n\nAs you can see, these four terminologies address different aspects of the data. While “EO data” refers to the data content (the Earth system), “geospatial data” addresses the location, and “remote sensing” and “satellite data” address the way of obtaining the data.\nThe Venn diagram below visualizes these definitions and shows overlaps and gaps between these concepts. Except for satellite data, which can be seen as a subset of remote sensing data, all other concepts have overlaps and gaps with each other.\n\n\n\nEO Venn diagram. Source: Abel and Jünger 2024\n\n\nLet’s disentangle these overlaps a bit further:\n\nA: The area A captures all concepts discussed above - Georeferenced Earth observation data derived from satellite sensors. An example for this are nighttime lights 🌇. Night lights are highly informative for social scientists to measure population growth, electrification and light pollution, the expansion of urban areas, the impacts of natural events, and economic activity. An important satellite for nighttime lights is the NASA/NOAA Suomi National Polar-orbiting Partnership (Suomi NPP), which was launched in 2011. On board of Suomi NPP, the Visible Infrared Imaging Radiometer Suite (VIIRS) instrument observes nighttime lights with the day night band (DNB). Luckily for us, we do not have to handle this raw data. NASA’s Black Marble project offers various pre-processed products with high spatial and temporal resolution and temporal coverage since 2012. The data products are free and open access. R user benefit from the BlackMarbleR-package, which is a user-friendly interface to Black Marble data.\n\nB: Area B is similar to A except that this data is derived from remote sensing OTHER than satellite data. There are, in fact, other platforms for sensors, like aircrafts or drones. For example, data on gases in the atmosphere are collected that way. Often, airborne data has a smaller geographical coverage and is more case specific. Furthermore, it is much more often proprietary and costly. For example, commercial providers offer airborne measurements of GHG emissions on a very high spatial resolution (&lt;1m) to estimate point-source emissions of factories and power plants.\nC: C represents EO data which is georeferenced but NOT derived from remote sensing. There are several alternatives to EO data generation: Ground-based sensors (called “in-situ”) are widespread sources of EO data. Weather stations or local air quality sensors are common examples. Projects like Google’s StreetView can also be classified here. But C also captures a data source which is very familiar to social scientists: Field surveys and survey-based methods. Biodiversity monitoring and soil quality projects often employ these methods. This work is not necessarily performed by experts only. Citizen science projects have contributed immensely to mapping local biodiversity, air quality, or land use: For example, scientists work together with “laypeople” to map bird populations like the RSPB in the UK with the annual Big Garden Birdwatch. Check out the European Citizen Science Association website to learn more about this form of data generation.\nD and E: The two areas D and E deviate from the previous concepts because they are NOT EO data. D captures all geodata which is not derived from remote sensing and does not constitute EO data. This is primarily data which is human-centered - many examples of which are familiar to social scientists: Georeferenced variables on socio-demographics or economic indicators like Census data, electoral outcomes, or environmental attitudes and behavior. Examples for E, which represents data gathered based on remote sensing but is NOT classified as Earth observation, are space exploration and astronomy 🌌. Imaging of Mars’ surface is such an example. Future generations of social scientists might be concerned with interplanetary societal issues - (un)fortunately for us, we will focus on societies from planet Earth 🚀.\n\nAs you can see, the main areas when it comes to EO data are A, B, and C. As social scientists interested in working with EO data, D is similarly relevant: Often, we are interested in spatially linking our social indicators (D) with EO data. We therefore see these as two sides of the same coin. The geolocation represents the functional link between our social indicators and the Earth system context.\nThere are a few additional areas in the Venn diagram which we have not highlighted. These are the fields which do not overlap with the circle for “geodata”. Since we are focussing on geo-referenced data in this project, these parts will not be further considered in our tutorials.\nThe conceptual distinctions between the ways of obtaining data (in-situ, airborne, or satellite sensors) are important but often not clear cut in practice. Most of the time, we access processed (and/or simulated) data products 🎁, which are generated by integrating datastreams from various sensors to increase data quality. This is done to enhance accuracy, scope or resolution, helps to validate data, or to fill data gaps. We will explore a few common sources and indicators in the next chapter."
  },
  {
    "objectID": "eod_intro.html#relevant-sources-of-earth-observation-data",
    "href": "eod_intro.html#relevant-sources-of-earth-observation-data",
    "title": "Introduction to Earth observation data",
    "section": "Relevant sources of Earth observation data",
    "text": "Relevant sources of Earth observation data\n\n\n\nWhere do we get EO data? (AI generated image)\n\n\nEurope’s Earth Observation programme is called Copernicus. It is funded and managed by the European Commission and partners like the European Space Agency (ESA) and the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT). It has been operational since 2014 and provides free access to a wealth of satellite data from ESA’s “Sentinel” fleet. Copernicus combines data from satellites, ground-based as well as air- and sea-borne sensors to track the Earth system and provide this information largely free for all customers.\n\n\nAdditional information on the Copernicus programme\n\nCheck out this 5min video on the Copernicus programme.\n\nThe ESA describes Copernicus as the world’s most ambitious Earth observation program, which will be further expanded in the coming years. On the Copernicus homepage, the daily data collection is estimated at 12 terabytes. Given this complexity, Copernicus has separated its services for public usage along several thematic areas:\n\nAtmosphere 🏭: Copernicus Atmosphere Monitoring Service (CAMS)\nMarine 🐳: Copernicus Marine Service (CMEMS)\nLand 🏞: Copernicus Land Monitoring Service (CLMS)\nClimate change ❄️ 🌡: Copernicus Climate Change Service (C3S)\nEmergency ⛈ 🌪: Copernicus Emergency Management Service (CEMS).\n\n\n\n\nCopernicus infrastructure and data services. Source: https://www.copernicus.eu/en/accessing-data-where-and-how/conventional-data-access-hubs\n\n\nThis project focuses on the data provided by the Copernicus programme. However, this is not the only relevant source of EO data which you can consider for your projects. The US equivalent, for example, is based on the Landsat satellite programme, which is jointly operated by NASA and the US Geological Survey (USGS). Google’s Earth Engine Cloud Computing Platform catalogs an extensive selection of additional data sets from various sources."
  },
  {
    "objectID": "raster_data.html",
    "href": "raster_data.html",
    "title": "Working with raster data in R",
    "section": "",
    "text": "Let’s code (AI generated image)!"
  },
  {
    "objectID": "raster_data.html#r-set-up",
    "href": "raster_data.html#r-set-up",
    "title": "Working with raster data in R",
    "section": "",
    "text": "tba"
  },
  {
    "objectID": "raster_data.html#example-1",
    "href": "raster_data.html#example-1",
    "title": "Working with raster data in R",
    "section": "Example 1",
    "text": "Example 1\ntba"
  },
  {
    "objectID": "courses.html",
    "href": "courses.html",
    "title": "Courses and workshops",
    "section": "",
    "text": "GESIS Workshop “Advanced Geospatial Data Processing for Social Scientists”, April 28/29 2024\nGESIS Workshop “Introduction to Geospatial Techniques for Social Scientists in R”, April 09/10 2024"
  },
  {
    "objectID": "courses.html#upcoming",
    "href": "courses.html#upcoming",
    "title": "Courses and workshops",
    "section": "",
    "text": "GESIS Workshop “Advanced Geospatial Data Processing for Social Scientists”, April 28/29 2024\nGESIS Workshop “Introduction to Geospatial Techniques for Social Scientists in R”, April 09/10 2024"
  },
  {
    "objectID": "courses.html#past-courses",
    "href": "courses.html#past-courses",
    "title": "Courses",
    "section": "Past courses",
    "text": "Past courses\ntba"
  },
  {
    "objectID": "issp.html",
    "href": "issp.html",
    "title": "Case Study: Linking ISSP survey data",
    "section": "",
    "text": "The International Social Survey Programme (ISSP) is a cross-national programme conducting annual surveys on diverse topics like:\n\nthe role of government 🏛,\ninequality 💰,\nwork orientations 👷,\nthe environment 🏞,\nor national identity 🇫🇷 🇿🇦 🇮🇳.\n\nThe ISSP was established in 1984 by the founding members Australia, Germany, Great Britain, and the US. Currently, the ISSP has 44 member states. Since its foundation, over one million respondents have participated in the surveys of the ISSP. All datasets are publicly available and free of charge.\nFor this case study, we are working with the cumulation of the environment module of the ISSP, which integrates the four existing survey rounds on the environment (1993, 2000, 2010, and 2020). Let’s assume we are interested in the role of long-term climate change patterns on how they affect environmental attitudes on the country-level. We first show a “manual” approach of retrieving and processing the EO indicator before introducing an easy alternative with the gxc-package.\n\n\n\nIntegrating a global survey with EO data (AI generated image)."
  },
  {
    "objectID": "issp.html#loading-issp-data",
    "href": "issp.html#loading-issp-data",
    "title": "Case Study: Linking ISSP survey data",
    "section": "",
    "text": "tba\n\n# Packages ----------------------------------------------------------------\n\nlibrary(haven)\nlibrary(sjlabelled)\n\n\nAttache Paket: 'sjlabelled'\n\n\nDie folgenden Objekte sind maskiert von 'package:haven':\n\n    as_factor, read_sas, read_spss, read_stata, write_sas, zap_labels\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ forcats::as_factor()     masks sjlabelled::as_factor(), haven::as_factor()\n✖ dplyr::as_label()        masks ggplot2::as_label(), sjlabelled::as_label()\n✖ dplyr::filter()          masks stats::filter()\n✖ dplyr::lag()             masks stats::lag()\n✖ sjlabelled::read_sas()   masks haven::read_sas()\n✖ sjlabelled::read_spss()  masks haven::read_spss()\n✖ sjlabelled::read_stata() masks haven::read_stata()\n✖ sjlabelled::write_sas()  masks haven::write_sas()\n✖ sjlabelled::zap_labels() masks haven::zap_labels()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(vtable)\n\nLade nötiges Paket: kableExtra\n\nAttache Paket: 'kableExtra'\n\nDas folgende Objekt ist maskiert 'package:dplyr':\n\n    group_rows\n\nlibrary(rnaturalearth)"
  },
  {
    "objectID": "issp.html#spatial-linking",
    "href": "issp.html#spatial-linking",
    "title": "Case Study: Linking ISSP survey data",
    "section": "Spatial linking",
    "text": "Spatial linking\nThe manual approach described above is simplified by the gxc-package. You can use the poly_link-function to replicate the process in an easy way.\nWe need devtools to load the gxc-package.\n\n# Install and load required packages\n# required_packages &lt;- c(\"devtools\")\n# new_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\n# if(length(new_packages)) install.packages(new_packages)\n# lapply(required_packages, library, character.only = TRUE)\n# \n# # Load gxc package (locally for now)\n# devtools::load_all()\n\nWe add a date-variable to the dataset which records the last month of the focal period (December 2020).\n\n# # Create fixed date-variable\n# world$date_raw &lt;- \"12-2020\"\n\n\n\n\nSteps in the spatial linking process. Source: Jünger 2019\n\n\nThis was a relatively easy example where we link data on the country-level. Data with a more fine-grained georeferencing requires even more flexible approaches. The gxc-package allows this linking on a higher spatial resolution. The next example with the GLES Panel shows how to do it."
  },
  {
    "objectID": "issp.html#the-issp-data",
    "href": "issp.html#the-issp-data",
    "title": "Case Study: Linking ISSP survey data",
    "section": "",
    "text": "The International Social Survey Programme (ISSP) is a cross-national programme conducting annual surveys on diverse topics like:\n\nthe role of government 🏛,\ninequality 💰,\nwork orientations 👷,\nthe environment 🏞,\nor national identity 🇫🇷 🇿🇦 🇮🇳.\n\nThe ISSP was established in 1984 by the founding members Australia, Germany, Great Britain, and the US. Currently, the ISSP has 44 member states. Since its foundation, over one million respondents have participated in the surveys of the ISSP. All datasets are publicly available and free of charge.\nFor this case study, we are working with the cumulation of the environment module of the ISSP, which integrates the four existing survey rounds on the environment (1993, 2000, 2010, and 2020). Let’s assume we are interested in the role of long-term climate change patterns on how they affect environmental attitudes on the country-level. We first show a “manual” approach of retrieving and processing the EO indicator before introducing an easy alternative with the gxc-package.\n\n\n\nIntegrating a global survey with EO data (AI generated image)."
  },
  {
    "objectID": "issp.html#setup-and-loading-the-data",
    "href": "issp.html#setup-and-loading-the-data",
    "title": "Case Study: Linking ISSP survey data",
    "section": "Setup and loading the data",
    "text": "Setup and loading the data\nFor loading and wrangling with the data, we need some packages. Keep in mind to install (install.packages()) those packages first before loading with the library-function.\n\nlibrary(haven) # For working with SPSS datafiles\nlibrary(sjlabelled) # To remove labels from SPSS datafiles\nlibrary(tidyverse) # For so much\n\nWe are loading the SPSS-file and directly remove all labels.\n\nissp &lt;- haven::read_spss(\"./data/ZA8793_v1-0-0.sav\") |&gt; \n  sjlabelled::remove_all_labels()"
  },
  {
    "objectID": "issp.html#setup",
    "href": "issp.html#setup",
    "title": "Case Study: Linking ISSP survey data",
    "section": "Setup",
    "text": "Setup\nFor loading and wrangling with the data, we need some packages. Keep in mind to install (install.packages()) those packages first before loading with the library-function.\n\nlibrary(haven) # For working with SPSS datafiles\nlibrary(sjlabelled) # To remove labels from SPSS datafiles\nlibrary(tidyverse) # For so much\n\nAfter downloading the data file from the website, place it in your project folder. We are loading the SPSS-file and directly remove all labels.\n\nissp &lt;- haven::read_spss(\"./data/issp_env/ZA8793_v1-0-0.sav\") |&gt; \n  sjlabelled::remove_all_labels()\n\nLet’s clean the dataset. We want to investigate the relationship between the experience of extreme heat and climate change concern. Let’s subset the datafile to variables which are necessary for us and rename directly. We will keep the variables measuring the survey year, the country of residence of the respondent, and the climate change concern item (v42).\n\nissp &lt;- issp |&gt; \n  select(\n    year, \n    country, \n    concern = v42\n    )\n\nYou can check out the codebook of the dataset to find out the country names for the numeric values in the dataset. Let’s label it. We will also combine responses from Northern Ireland and Great Britain into “United Kingdom” and store it in a new variable.\n\nissp &lt;- issp |&gt; \n  mutate(\n    country = factor(country, levels=c(36, 40, 100, 124, 152, 158, 191, 203, \n                                       208, 246, 250, 276, 348, 352, 372, 376,\n                                       380, 392, 410, 428, 440, 484, 528, 554,\n                                       578, 608, 620, 643, 703, 705, 710, 724,\n                                       752, 756, 826, 840, 82602), \n                          labels=c(\"Australia\", \"Austria\", \"Bulgaria\", \"Canada\",\n                                   \"Chile\", \"Taiwan\", \"Croatia\", \"Czechia\",\n                                   \"Denmark\", \"Finland\", \"France\", \"Germany\",\n                                   \"Hungary\", \"Iceland\", \"Ireland\", \"Israel\",\n                                   \"Italy\", \"Japan\", \"South Korea\", \"Latvia\",\n                                   \"Lithuania\", \"Mexico\", \"Netherlands\",\n                                   \"New Zealand\", \"Norway\", \"Philippines\",\n                                   \"Portugal\", \"Russia\", \"Slovakia\",\n                                   \"Slovenia\", \"South Africa\", \"Spain\",\n                                   \"Sweden\", \"Switzerland\", \"Great Britain\", \n                                   \"USA\",\n                                   \"Northern Ireland\"))\n  )\n\nissp &lt;- issp |&gt; \n  mutate(\n    country_new = case_when(\n      country == \"Great Britain\" | country == \"Northern Ireland\" ~ \"United Kingdom\",\n      TRUE ~ country\n      ),\n    .after = country\n  )\n\nLet’s also reverse the concern-scale so that higher values indicate higher concern.\n\nissp &lt;- issp |&gt; \n  mutate(\n    concern = case_match(concern,\n                         1 ~ 5,\n                         2 ~ 4,\n                         3 ~ 3,\n                         4 ~ 2,\n                         5 ~ 1)\n  )\n\nThe dataset is ready for linking with our temperature data.\n\nhead(issp)\n\n  year   country country_new concern\n1 1993 Australia   Australia       4\n2 1993 Australia   Australia       4\n3 1993 Australia   Australia       5\n4 1993 Australia   Australia       3\n5 1993 Australia   Australia       3\n6 1993 Australia   Australia       3\n\n\nBefore we do that, let’s explore the distribution of climate concern across countries for the 2020 wave.\n\n# Define Likert-theme\nlikert_theme &lt;- theme_gray() +\n  theme(text = element_text(size = 12, face = \"bold\"),\n        plot.title = element_text(size = 13, face = \"bold\",\n                                  margin = margin(10, 0, 10, 0)), \n        plot.margin = unit(c(.4,0,.4,.4), \"cm\"),\n        plot.subtitle = element_text(face = \"italic\"),\n        legend.title = element_blank(),\n        legend.key.size = unit(1, \"line\"),\n        legend.background = element_rect(fill = \"grey90\"),\n        panel.grid = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks = element_blank(),\n        axis.title = element_blank(),\n        panel.background = element_blank(),\n        strip.text = element_text(size = 12, face = \"bold\"))\n\n# Let's look at 2020 only\nissp_2020 &lt;- issp |&gt; \n  filter(year == \"2020\")\n\n# Plot\nissp_2020 |&gt;\n  filter(!is.na(concern)) |&gt;\n  mutate(country_new = forcats::fct_reorder(country_new, concern, \n                                            .fun=mean, .desc=FALSE)) |&gt; \n  arrange(country_new) |&gt;\n  group_by(country_new, concern) |&gt;\n  summarize(count = n()) |&gt;\n  group_by(country_new) |&gt; \n  mutate(prop_value = count / sum(count)) |&gt;\n  ggplot() +\n  geom_bar(mapping = aes(x = country_new,\n                         y = prop_value,\n                         fill = forcats::fct_rev(factor(concern))),\n           position = \"fill\",\n           stat = \"identity\")+\n  geom_text(aes(x = country_new, y = prop_value, label = round(100*prop_value)), \n            position = position_stack(vjust = 0.5), \n            fontface = \"bold\") +\n  scale_fill_brewer(type = \"div\", palette = \"PRGn\", direction = -1,\n                    labels = c(\"5 - High concern\", \"4\", \"3\", \"2\", \"1 - No concern\")) +\n  coord_flip() +\n  likert_theme +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_legend(reverse = TRUE, nrow =1))"
  },
  {
    "objectID": "eod_intro.html#applications-in-the-social-sciences",
    "href": "eod_intro.html#applications-in-the-social-sciences",
    "title": "Introduction to Earth observation data",
    "section": "Applications in the social sciences",
    "text": "Applications in the social sciences\nEO data is not just utilized in the Earth system sciences. A growing interest in economics and the social sciences in Earth observation data has increased the number of publications in recent years, which integrate, in one way or another, EO data in the research design.\nWe have identified 6 major topics in the social sciences which have benefited from this data source in the past:\n\nEnvironmental social sciences 🌱,\nConflict and peace research 🕊,\nPolitical attitudes and behavior 🗳,\nPolicy studies 📜,\nEconomic development and inequality 📈, and\nPublic health 💪.\n\nThe environmental social sciences are a growing research field at the intersection between the Earth system and societies. One particular topic, the role of extreme weather events for people’s environmental attitudes and behavior, has especially benefited from a growing data availability of EO data. A noteworthy study by Hoffmann et al. (2022) analyses how the experience of climate anomalies and extremes influences environmental attitudes and vote intention in Europe. They integrate sources from 1. harmonized Eurobarometer surveys, 2. EU parliamentary electoral data, and 3. climatological data and aggregate it on the regional level (NUTS2 and NUTS3). The climatological data is derived from C3S ERA5 reanalysis and is utilized to calculate temperature anomalies and extremes based on the reference period 1971-2000. Their findings suggest an effect of temperature anomalies (heat, “dry spell”) on environmental concern and vote intention.\nEconomists and social scientists who study economic development and inequality exploit EO data in various forms to operationalize independent variables such as drivers and barriers to development (e.g. droughts) as well as dependent variables (e.g. night lights as proxies for economic activity or the quality of rooftops as development indicator). García-León et al. (2021), for example, investigate historical and future economic impacts of recent heatwaves (2003, 2010, 2015, 2018) in Europe. They combine and regionally-aggregate 1. heatwave data with 2. population data, 3. worker productivity data, and 4. economic accounts from Eurostat. They utilize the C3S ERA5 data to calculate heatwaves based on the reference period 1981-2010. Their findings indicate total estimated damages attributed to heatwaves to 0.3-0.5% of European GDP with high a geospatial variation (GDP impacts beyond 1% in vulnerable regions). Jean et al. (2016) show how nighttime maps can be utilized as estimates of household consumption and assets. Economic indicators are hard to measure in poorer countries - satellite imagery could be an alternative proxy for it. The authors integrate 1. survey data (World Bank’s Living Standards Measurement Surveys - LSMS; and Demographic and Health Surveys - DHS) with 2. nighttime light data in five African countries - Nigeria, Tanzania, Uganda, Malawi, and Rwanda. They utilize ML approaches for image feature extraction on daytime satellite images from Google Static Maps and nighttime lights from US DMSP. They find that their model can explain up to 75% of variation in local-level economic outcomes.\nBeyond single research projects, EO data is increasingly integrated in official statistical accounts. For example, the German Federal Statistical Office (DESTATIS) is currently evaluating the usage of remote sensing and satellite data for official statistical accounts in several projects as part of their experimental statistics. The agency explores, among other things, whether the number of ships and containers in harbors or inland waters can be used as proxies for trading activities or production figures or whether the occupancy of parking spaces adjacent to shops are indicative of sales figures (for more information, see their project on “Smart business cycle statistics based on satellite data”. In the long-term, they plan to integrate indicators, like vehicle manufacturing indices, into GDP flash estimates and nowcasting (see - Satellite-based early estimate of short-term economic development). For the establishment of the register census in 2031, they furthermore develop algorithms for the identification of buildings based on satellite and airborne imagery (Remote sensing data and artifical intelligence in the register census)\nEarth observation is a growing field. Over the next few years, many more satellites will be put into orbit to increase data quality, resolution and range. Better data quality will further increase the possibilities for usage in the social sciences. Furthermore, novel applications of machine learning (ML) on satellite and aerial imagery expand the potential of EO data for the construction of new indicators. While lacking data availability, until recently, had severely limited use cases in the social sciences, these innovations will drive social scientists to tackle new research questions. Consider, for example, Stanford’s DeepSolar, which maps solar PV installations in the US at an unprecedented level of detail and enables researchers to study household-level adoption and diffusion of renewable energy technologies."
  },
  {
    "objectID": "eod_intro.html#sources",
    "href": "eod_intro.html#sources",
    "title": "Introduction to Earth Observation data",
    "section": "Sources",
    "text": "Sources"
  },
  {
    "objectID": "courses.html#past-events",
    "href": "courses.html#past-events",
    "title": "Courses and workshops",
    "section": "Past events",
    "text": "Past events\n\nInternational Expert Workshop on “Earth Observation Data in the Social Sciences”, 27-29 November 2023 at GESIS - Leibniz Institute for Social Sciences.\nMethods Primer on “Working with Earth Observation data in R”, 31 October 2024, MethodsNET Launch Conference at UCLouvain, Belgium. The slides are accessible here.\nGESIS Workshop on “Introduction to Geospatial Techniques for Social Scientists in R”, including matters relevant for Earth Observation Data, April 23/24 2024. All the materials are accessible here\nInternational Expert Workshop on “Earth Observation Data in the Social Sciences”, 13-15 December 2023 at GESIS - Leibniz Institute for Social Sciences. Check out a few impressions from the workshop in this video."
  },
  {
    "objectID": "package.html#introduction",
    "href": "package.html#introduction",
    "title": "Package",
    "section": "",
    "text": "The unique feature of the tool should be the possibility of carrying out both geographically and temporally high-resolution queries of data from Copernicus and other Earth observation data sources, which at the same time function efficiently on simple workstations albeit large amounts of data. Our tested workflow development has identified five major levers: indicator type, indicator intensity, focal time period, baseline time period, and spatial buffer. Flexibility on these five attributes should be maximized for users. The tool also offers the functionality to automatically derive spatio-temporal links with other georeferenced data (e.g., surveys, digital behavioral data).\n\n\n\n\n\n\n\nUsers should benefit from the core variables integrated into the interface for social research. Preparatory work is currently being carried out to select appropriate indicators from the corpus of indicators offered by data providers and conceptualize the data integration logic. Furthermore, in exchange with users and other stakeholders, we compile data products from EOD that are particularly relevant for social scientists. Examples include data on local air quality and pollutants, extreme weather events, or land use changes. The main data providers will be the Copernicus Monitoring Services on Climate Change, Atmosphere, and Land.\n\n\n\nMajor attributes for indicator specification. Source: Abel and Jünger 2024\n\n\nGESIS strongly supports the FAIR data principles and Open Data. The selection of R as the programming language for the tool supports open-source infrastructure development and shareability, as well as quality control via online repositories. Publishing the R scripts for data management and analysis ensures the reproducibility of all research steps."
  },
  {
    "objectID": "issp.html#eo-indicators",
    "href": "issp.html#eo-indicators",
    "title": "Case Study: Linking ISSP survey data",
    "section": "EO indicators",
    "text": "EO indicators\nWe want to investigate whether temperature anomalies in the year of the survey, in comparison to a long running average, are associated with climate change concern. For this example, we will only focus on the 2020 survey wave but the process can be replicated for each survey round. The visualization below helps us to conceptualize our climate indicator:\n\nIndicator: Temperature - annual average\nIntensity: Anomaly (mean deviation)\nFocal time period: 2020\nBaseline period: 1961-1990\nSpatial buffer: Country\n\n\n\n\nMajor attributes for indicator specification. Source: Abel and Jünger 2024\n\n\nThe ERA5-Land Reanalyis from the Copernicus Climate Change Service is a suitable data product for this temperature indicator. It records observations on air temperature at 2 meters above the surface from 1950 onwards, has a spatial resolution of 0.1x0.1degrees and a global spatial coverage.\nIn order to access the data, we need an ECMWF-account. Utilizing the ecmwfr-package, we can access the data directly in R."
  },
  {
    "objectID": "eod_intro.html#data-quality",
    "href": "eod_intro.html#data-quality",
    "title": "Introduction to Earth observation data",
    "section": "Data quality",
    "text": "Data quality\nData quality\nThe German Federal Statistical Office (DESTATIS) is currently evaluating the usage of remote sensing data for official statistical accounts in several projects as part of their experimental statistics.\n\nSmart business cycle statistics based on satellite data\nRemote sensing data and artifical intelligence in the register census\nSatellite-based early estimate of short-term economic development"
  },
  {
    "objectID": "about.html#gesis",
    "href": "about.html#gesis",
    "title": "About",
    "section": "",
    "text": "GESIS is one of the world’s leading infrastructure institutions for the social sciences and supports researchers with expertise and infrastructure services at all levels of their research projects. We help to ensure that socially relevant questions can be answered on the basis of reliable data, the latest scientific methods and research information. We help with study planning and data collection, provide high-quality research data, support with data processing and analysis and finally with archiving the data.\nhttps://www.gesis.org/en"
  },
  {
    "objectID": "about.html#team",
    "href": "about.html#team",
    "title": "About",
    "section": "Team",
    "text": "Team\n\nDr. Dennis Abel\nDr. Stefan Jünger"
  },
  {
    "objectID": "raster_data.html#what-exactly-are-raster-data",
    "href": "raster_data.html#what-exactly-are-raster-data",
    "title": "Working with raster data in R",
    "section": "",
    "text": "Let’s code (AI generated image)!"
  },
  {
    "objectID": "raster_data.html#metadata",
    "href": "raster_data.html#metadata",
    "title": "Working with raster data in R",
    "section": "Metadata",
    "text": "Metadata\n\nInformation about geometries is globally stored\nthey are the same for all observations\ntheir location in space is defined by their cell location in the data table\nWithout this information, raster data were simple image files"
  },
  {
    "objectID": "raster_data.html#important-metadata",
    "href": "raster_data.html#important-metadata",
    "title": "Working with raster data in R",
    "section": "Important Metadata",
    "text": "Important Metadata\n\nRaster Dimensions\n\nnumber of columns, rows, and cells\n\nExtent\n\nSimilar to bounding box in vector data\n\nResolution\n\nthe size of each raster cell\n\nCoordinate reference system\n\ndefines where on the earth’s surface the raster layer lies"
  },
  {
    "objectID": "raster_data.html#all-beginnings-are-easy",
    "href": "raster_data.html#all-beginnings-are-easy",
    "title": "Working with raster data in R",
    "section": "All Beginnings Are… Easy!",
    "text": "All Beginnings Are… Easy!\n\nterra::rast()\n\nclass       : SpatRaster \ndimensions  : 180, 360, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84)"
  },
  {
    "objectID": "raster_data.html#feed-with-data",
    "href": "raster_data.html#feed-with-data",
    "title": "Working with raster data in R",
    "section": "Feed With Data",
    "text": "Feed With Data\n\ninput_data &lt;- \n  sample(1:100, 16) |&gt; \n  matrix(nrow = 4)\n\nraster_layer &lt;- terra::rast(input_data)\n\nraster_layer\n\nclass       : SpatRaster \ndimensions  : 4, 4, 1  (nrow, ncol, nlyr)\nresolution  : 1, 1  (x, y)\nextent      : 0, 4, 0, 4  (xmin, xmax, ymin, ymax)\ncoord. ref. :  \nsource(s)   : memory\nname        : lyr.1 \nmin value   :     5 \nmax value   :    91"
  },
  {
    "objectID": "raster_data.html#plotting",
    "href": "raster_data.html#plotting",
    "title": "Working with raster data in R",
    "section": "Plotting",
    "text": "Plotting\n\nterra::plot(raster_layer)"
  },
  {
    "objectID": "raster_data.html#file-formatsextensions",
    "href": "raster_data.html#file-formatsextensions",
    "title": "Working with raster data in R",
    "section": "File Formats/Extensions",
    "text": "File Formats/Extensions\n\nGtiff/GeoTiff\nJPEG2000\nGRIB\n.grd\nnetCDF\n…\nsometimes, raster data come even in a text format, such as CSV"
  },
  {
    "objectID": "raster_data.html#implementations-in-r",
    "href": "raster_data.html#implementations-in-r",
    "title": "Working with raster data in R",
    "section": "Implementations in R",
    "text": "Implementations in R\nAFAIK terra is the most commonly used package for raster data in R.\nSome other developments, e.g., in the stars package, also implement an interface to simple features in sf.\nThe terra package also helps to use more elaborate zonal statistics. The same holds for the spatstat package."
  },
  {
    "objectID": "raster_data.html#basic-raster-operations",
    "href": "raster_data.html#basic-raster-operations",
    "title": "Working with raster data in R",
    "section": "Basic Raster Operations",
    "text": "Basic Raster Operations"
  },
  {
    "objectID": "raster_data.html#loading-raster-tiffs-temperature-data",
    "href": "raster_data.html#loading-raster-tiffs-temperature-data",
    "title": "Working with raster data in R",
    "section": "Loading Raster Tiffs (temperature data)",
    "text": "Loading Raster Tiffs (temperature data)\n\ntemp_6_2019 &lt;-\n  terra::rast(\"./data/temp_6_2019.tif\")\n\ntemp_7_2019 &lt;-\n  terra::rast(\"./data/temp_7_2019.tif\")\n\ntemp_6_2019\n\nclass       : SpatRaster \ndimensions  : 561, 1440, 1  (nrow, ncol, nlyr)\nresolution  : 0.25, 0.25  (x, y)\nextent      : -180.125, 179.875, -56.125, 84.125  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : temp_6_2019.tif \nname        : SFC (Ground or water surface); 2 metre temperature [C] \nmin value   :                                               258.1914 \nmax value   :                                               313.1426"
  },
  {
    "objectID": "raster_data.html#compare-layers-by-plotting",
    "href": "raster_data.html#compare-layers-by-plotting",
    "title": "Working with raster data in R",
    "section": "Compare Layers by Plotting",
    "text": "Compare Layers by Plotting\n\nterra::plot(temp_6_2019)\n\n\n\n\n\n\n\n\n\nterra::plot(temp_7_2019)"
  },
  {
    "objectID": "raster_data.html#simple-statistics",
    "href": "raster_data.html#simple-statistics",
    "title": "Working with raster data in R",
    "section": "Simple Statistics",
    "text": "Simple Statistics\nWorking with raster data is straightforward - quite speedy - yet not as comfortable as working with sf objects\nFor example, to calculate the mean we would use:\n\nterra::global(temp_6_2019, fun = \"mean\", na.rm = TRUE)\n\n                                                           mean\nSFC (Ground or water surface); 2 metre temperature [C] 289.9985"
  },
  {
    "objectID": "raster_data.html#combining-raster-layers-to-calculate-new-values",
    "href": "raster_data.html#combining-raster-layers-to-calculate-new-values",
    "title": "Working with raster data in R",
    "section": "Combining Raster Layers to Calculate New Values",
    "text": "Combining Raster Layers to Calculate New Values\nAlthough raster data are simple data tables, working with them is a bit different compared to, e.g., simple features.\n\ntemp_6_2019_celsius &lt;-\n  temp_6_2019 - 273.15\n\ntemp_7_2019_celsius &lt;-\n  temp_7_2019 - 273.15\n\ntemp_6_2019_celsius\n\nclass       : SpatRaster \ndimensions  : 561, 1440, 1  (nrow, ncol, nlyr)\nresolution  : 0.25, 0.25  (x, y)\nextent      : -180.125, 179.875, -56.125, 84.125  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nvarname     : temp_6_2019 \nname        : SFC (Ground or water surface); 2 metre temperature [C] \nmin value   :                                              -14.95859 \nmax value   :                                               39.99258"
  },
  {
    "objectID": "raster_data.html#combining-raster-layers-to-calculate-new-values-1",
    "href": "raster_data.html#combining-raster-layers-to-calculate-new-values-1",
    "title": "Working with raster data in R",
    "section": "Combining Raster Layers to Calculate New Values",
    "text": "Combining Raster Layers to Calculate New Values\nAlthough raster data are simple data tables, working with them is a bit different compared to, e.g., simple features.\n\ntemp_diff &lt;-\n  temp_7_2019 - temp_6_2019\n\ntemp_diff\n\nclass       : SpatRaster \ndimensions  : 561, 1440, 1  (nrow, ncol, nlyr)\nresolution  : 0.25, 0.25  (x, y)\nextent      : -180.125, 179.875, -56.125, 84.125  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource(s)   : memory\nvarname     : temp_7_2019 \nname        : SFC (Ground or water surface); 2 metre temperature [C] \nmin value   :                                              -6.845703 \nmax value   :                                              12.255859"
  },
  {
    "objectID": "raster_data.html#subsetting-raster-layers",
    "href": "raster_data.html#subsetting-raster-layers",
    "title": "Working with raster data in R",
    "section": "‘Subsetting’ Raster Layers",
    "text": "‘Subsetting’ Raster Layers\nWe can subset vector data by simply filtering for specific attribute values. For example, to subset a world map only to Belgium, we can use the Tidyverse for sf data:\n\nworld &lt;- rnaturalearth::ne_countries(\n  scale = \"medium\", \n  returnclass = \"sf\")\n\n# Subset to relevant variables\nworld &lt;- world |&gt; \n  dplyr::select(admin, geometry)\n\n# Subset to Belgium\nbelgium &lt;- world |&gt; \n  dplyr::filter(admin == \"Belgium\")\n\nsf::st_geometry(belgium)\n\nGeometry set for 1 feature \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.524902 ymin: 49.51089 xmax: 6.364453 ymax: 51.49111\nGeodetic CRS:  WGS 84\n\n\nMULTIPOLYGON (((4.226172 51.38647, 4.304492 51...."
  },
  {
    "objectID": "raster_data.html#cropping",
    "href": "raster_data.html#cropping",
    "title": "Working with raster data in R",
    "section": "Cropping",
    "text": "Cropping\nCropping is a method of cutting out a specific slice of a raster layer based on an input dataset or geospatial extent, such as a bounding box. Cropping reduces the spatial extent of a raster to a specified rectangular bounding box.\n\ncropped_temp_6_2019 &lt;-\n  terra::crop(temp_6_2019_celsius, belgium)"
  },
  {
    "objectID": "raster_data.html#masking",
    "href": "raster_data.html#masking",
    "title": "Working with raster data in R",
    "section": "Masking",
    "text": "Masking\nMasking is similar to cropping, yet values outside the extent are set to missing values (NA). Masking creates a precise match between the spatial extent of your shape and the raster values.\n\nmasked_temp_6_2019 &lt;-\n  raster::mask(temp_6_2019_celsius, terra::vect(belgium))"
  },
  {
    "objectID": "raster_data.html#combining-cropping-and-masking",
    "href": "raster_data.html#combining-cropping-and-masking",
    "title": "Working with raster data in R",
    "section": "Combining Cropping and Masking",
    "text": "Combining Cropping and Masking\nCropping first and masking afterwards combines both processes.\n\ntemp_6_2019_belgium &lt;-\n  terra::crop(temp_6_2019_celsius, belgium) |&gt; \n  raster::mask(terra::vect(belgium))\n\n\n\nBreaking News: tmap 3.x is retiring. Please test v4, e.g. with\nremotes::install_github('r-tmap/tmap')"
  },
  {
    "objectID": "raster_data.html#raster-extraction-zonal-statistics",
    "href": "raster_data.html#raster-extraction-zonal-statistics",
    "title": "Working with raster data in R",
    "section": "Raster Extraction / Zonal statistics",
    "text": "Raster Extraction / Zonal statistics"
  },
  {
    "objectID": "raster_data.html#sampling-of-some-points",
    "href": "raster_data.html#sampling-of-some-points",
    "title": "Working with raster data in R",
    "section": "Sampling of some points",
    "text": "Sampling of some points\n\nrandom_points &lt;-\n  temp_6_2019_belgium |&gt; \n  terra::spatSample(size = 10, na.rm = TRUE, as.points = TRUE) |&gt; \n  sf::st_as_sf() |&gt; \n  dplyr::select(-1)\n\n\nplot(random_points)"
  },
  {
    "objectID": "raster_data.html#extract-information-from-rasters",
    "href": "raster_data.html#extract-information-from-rasters",
    "title": "Working with raster data in R",
    "section": "Extract Information From Rasters",
    "text": "Extract Information From Rasters\nRaster data are helpful when we aim to - apply calculations that are the same for all geometries in the dataset - extract information from raster fast and efficient\n\nlibrary(tmap)\n\ntm_shape(temp_6_2019_belgium) +\n  tm_raster() +\n  tm_shape(belgium) +\n  tm_borders(col = \"black\", lwd = 2) +\n  tm_shape(random_points) +\n  tm_dots(size = .25)"
  },
  {
    "objectID": "raster_data.html#raster-extraction",
    "href": "raster_data.html#raster-extraction",
    "title": "Working with raster data in R",
    "section": "Raster Extraction",
    "text": "Raster Extraction\nTo extract the raster values at a specific point by location, we use the following:\n\nterra::extract(temp_6_2019_belgium, random_points, ID = FALSE)\n\n   SFC (Ground or water surface); 2 metre temperature [C]\n1                                                18.46133\n2                                                17.70742\n3                                                18.18594\n4                                                18.10195\n5                                                19.13711\n6                                                17.81680\n7                                                18.86367\n8                                                18.66055\n9                                                16.70938\n10                                               18.11953"
  },
  {
    "objectID": "raster_data.html#add-results-to-existing-dataset",
    "href": "raster_data.html#add-results-to-existing-dataset",
    "title": "Working with raster data in R",
    "section": "Add Results to Existing Dataset",
    "text": "Add Results to Existing Dataset\nThis information can be added to an existing dataset (our points in this example):\n\nrandom_points &lt;-\n  random_points |&gt; \n  dplyr::mutate(\n    temp_value = \n      as.vector(\n        terra::extract(temp_6_2019_belgium, random_points, ID = FALSE, raw = TRUE)\n      )\n  )\n\nrandom_points\n\nSimple feature collection with 10 features and 1 field\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 3 ymin: 49.75 xmax: 5.25 ymax: 51.25\nGeodetic CRS:  WGS 84\n             geometry temp_value\n1   POINT (4.5 51.25)   18.46133\n2  POINT (3.25 50.75)   17.70742\n3        POINT (4 51)   18.18594\n4     POINT (4 50.75)   18.10195\n5     POINT (5.25 51)   19.13711\n6  POINT (3.75 51.25)   17.81680\n7        POINT (5 51)   18.86367\n8  POINT (4.75 51.25)   18.66055\n9     POINT (3 51.25)   16.70938\n10    POINT (5 49.75)   18.11953"
  },
  {
    "objectID": "raster_data.html#more-elaborated-spatial-buffers",
    "href": "raster_data.html#more-elaborated-spatial-buffers",
    "title": "Working with raster data in R",
    "section": "More Elaborated: Spatial Buffers",
    "text": "More Elaborated: Spatial Buffers\nSometimes, extracting information 1:1 is not enough - too narrow - missing information about the surroundings of a point\n\ntm_shape(temp_6_2019_belgium) +\n  tm_raster() +\n  tm_shape(\n    sf::st_buffer(random_points, 5000) \n  ) +\n  tm_dots(size = .1) +\n  tm_borders()"
  },
  {
    "objectID": "raster_data.html#a-closer-look",
    "href": "raster_data.html#a-closer-look",
    "title": "Working with raster data in R",
    "section": "A Closer Look",
    "text": "A Closer Look\n\nJünger, 2021"
  },
  {
    "objectID": "raster_data.html#raster-stacks",
    "href": "raster_data.html#raster-stacks",
    "title": "Working with raster data in R",
    "section": "Raster Stacks",
    "text": "Raster Stacks\nSo far, raster data have been unidimensional: we only had one attribute for each dataset.\nBut they can also be stacked:\n\ntemp_stack &lt;- c(temp_6_2019, temp_7_2019)\n\ntemp_stack\n\nclass       : SpatRaster \ndimensions  : 561, 1440, 2  (nrow, ncol, nlyr)\nresolution  : 0.25, 0.25  (x, y)\nextent      : -180.125, 179.875, -56.125, 84.125  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsources     : temp_6_2019.tif  \n              temp_7_2019.tif  \nnames       : SFC (Ground or ~temperature [C], SFC (Ground or ~temperature [C] \nmin values  :                        258.1914,                        258.7051 \nmax values  :                        313.1426,                        314.7812"
  },
  {
    "objectID": "raster_data.html#magic-of-data-cubes-in-the-stars-package",
    "href": "raster_data.html#magic-of-data-cubes-in-the-stars-package",
    "title": "Working with raster data in R",
    "section": "Magic of Data Cubes In the stars Package",
    "text": "Magic of Data Cubes In the stars Package\n\nhttps://raw.githubusercontent.com/r-spatial/stars/master/images/cube2.png"
  },
  {
    "objectID": "raster_data.html#what-exactly-are-raster-data-1",
    "href": "raster_data.html#what-exactly-are-raster-data-1",
    "title": "Working with raster data in R",
    "section": "What Exactly Are Raster Data?",
    "text": "What Exactly Are Raster Data?\n\nHold information on (most of the time) evenly shaped grid cells\nBasically, a simple data table\neach cell represents one observation"
  },
  {
    "objectID": "quality_criteria.html",
    "href": "quality_criteria.html",
    "title": "EO data quality criteria",
    "section": "",
    "text": "Earth observation data often comes in raster-format, sometimes in vector-format. In order to assess the quality of a specific EO product and determine wether it is suitable for our specific research purpose, we need to be aware of a few terminologies. These are explained below. For researchers who regularly work with geographical data, these should sound familiar.\n\n\n\nCreating high quality maps (AI generated image)."
  },
  {
    "objectID": "quality_criteria.html#major-attributes-of-eo-datasets",
    "href": "quality_criteria.html#major-attributes-of-eo-datasets",
    "title": "EO data quality criteria",
    "section": "Major attributes of EO datasets",
    "text": "Major attributes of EO datasets\nThe list below summarizes the most important data quality criteria for EO data:\n\nVariable: Measuring the specific indicator of interest. This could be for example values of nighttime lights, GHG emissions like CO2 or CH4 from the Emission Database for Global Atmospheric Research (EDGAR), or landscape values like NDVI.\nCRS and projection: A Coordinate Reference System (CRS) is the framework for identifying locations. It defines how the two-dimensional map refers to actual places on the Earth’s surface. A CRS typically includes a datum (which defines the coordinate axes) and either uses geographic coordinates (latitude and longitude) or projected coordinates. Projected coordinates are based on mathematical methods to transform the Earth’s three-dimensional surface into a two-dimensional plane (the “map”). Any projection inevitably distorts some properties of shape, area, distance, or direction. Different projections focus on preserving different characteristics.\nSpatial resolution: The spatial resolution reports the granularity of the data. Finer granularity stores more detailed information. For gridded data, and depending on the measurement framework, the spatial resolution is either reported in angular units (degrees) that represent longitude and latitude in a spherical coordinate system or square (kilo-)meters projected into Cartesian coordinate systems. High-resolution airborne monitoring, for example, achieves a spatial resolution at or below 1x1m. Publicly available data products derived from satellite data are often much more coarse with values of 1x1km or larger, like the Open-source Data Inventory for Anthropogenic CO2 (ODIAC). The spatial resolution of georeferenced data which is stored in vector-format refers to the detailedness and aggregation level of data in points, lines, and polygons. For example, is pollution data stored for each individual emitting factory in point location or as regional or national averages?\nSpatial coverage: The spatial coverage (or scope/extent) refers to the extensiveness of data across the globe. The delineation of coverage can be based on the exact spatial extent of gridded data, or on administrative units like regions and countries.\nTemporal resolution: Similar to the spatial resolution, the temporal resolution records the intervalls at which information is stored. (Near-)real-time data from public sources have hourly or daily intervals. Many datasets also provide monthly or yearly averages.\nTemporal coverage: The temporal coverage (or scope/extent) means the time period, which is covered by the dataset. Given that many remote sensing and satellite programs have emerged only recently, the temporal coverage of observational data is limited. Fortunately, reanalysis programs, simulation studies, and data fusion projects have extended the historical coverage of many EO indicators considerably. Check out, for example, the Community Emissions Data System (CEDS), which has produced annual estimates of several anthropogenic emission species over the entire industrial period from 1750!\nMeasurement type: EO data can be collected through different channels. Bottom-up approaches like direct reporting of emissions based on in-situ sensors or estimations of point sources can offer high accuracy but often lack broad coverage. In contrast, top-down approaches based on remote sensing can generate global coverage at risk of accuracy issues. Many data products integrate different data sources and apply additional techniques to increase data quality through simulation studies, data fusion, or reanalysis.\nProcessing level: The processing level describes how much pre-processing of the original raw data has been involved. Levels are often classified from 0 (raw) to 4 (model output). As endusers, we are normally accessing data products at level 2 or 3, which represent derived geophysical variables mapped on uniform space-time grid scales and checked for completeness and consistency. For more information, see NASA’s description here."
  },
  {
    "objectID": "catalogue.html",
    "href": "catalogue.html",
    "title": "Indicator catalogue",
    "section": "",
    "text": "This page summarizes the supported catalogue of indicators. The current version accesses datasets from:\n1. Copernicus Climate Change Service (C3S):\n\nERA5-Land Monthly Averaged Data (1950–Present)\nERA5 Monthly Averaged Data on Single Levels (1940–Present)"
  },
  {
    "objectID": "catalogue.html#dataset-attributes",
    "href": "catalogue.html#dataset-attributes",
    "title": "Indicator catalogue",
    "section": "Dataset Attributes",
    "text": "Dataset Attributes\n\n\n\n\n\n\n\n\n\n\nAttribute\nERA5.Land.Monthly.Averaged.Data\nERA5.Monthly.Averaged.Data.on.Single.Levels\n\n\n\n\nProjection\nWGS84 (EPSG:4326)\nWGS84 (EPSG:4326)\n\n\nSpatial Resolution\n~9 km (~0.1°)\n~31 km (~0.25°)\n\n\nSpatial Coverage (Extent)\nGlobal (-90° to 90°, -180° to 180°)\nGlobal (-90° to 90°, -180° to 180°)\n\n\nTemporal Resolution\nMonthly\nMonthly\n\n\nTemporal Coverage\n1950 to present\n1940 to present\n\n\nUpdate Frequency\nMonthly\nMonthly"
  },
  {
    "objectID": "catalogue.html#accessed-datasets",
    "href": "catalogue.html#accessed-datasets",
    "title": "Indicator catalogue",
    "section": "",
    "text": "This page summarizes the supported catalogue of indicators. The current version accesses datasets from:\n1. Copernicus Climate Change Service (C3S):\n\nERA5-Land Monthly Averaged Data (1950–Present)\nERA5 Monthly Averaged Data on Single Levels (1940–Present)"
  },
  {
    "objectID": "catalogue.html#supported-indicators",
    "href": "catalogue.html#supported-indicators",
    "title": "Indicator catalogue",
    "section": "Supported indicators",
    "text": "Supported indicators\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\nERA5.Land\nERA5.Single.Levels\n\n\n\n\n2m_temperature\nAir temperature at 2 meters above the surface (°C)\n✔\n✔\n\n\ntotal_precipitation\nTotal accumulated precipitation (m)\n✔\n\n\n\n10m_u_component_of_wind\nEastward component of wind speed at 10 meters (m/s)\n✔\n✔\n\n\n10m_v_component_of_wind\nNorthward component of wind speed at 10 meters (m/s)\n✔\n✔\n\n\n10m_wind_speed\nWind speed magnitude at 10 meters (m/s)\n\n✔\n\n\ntotal_cloud_cover\nFraction of sky covered by clouds (0-1)\n\n✔\n\n\nleaf_area_index_high_vegetation\nLeaf area index for high vegetation (m²/m²)\n✔\n✔\n\n\nleaf_area_index_low_vegetation\nLeaf area index for low vegetation (m²/m²)\n✔\n✔\n\n\nsnowfall\nTotal accumulated snowfall (m)\n✔"
  },
  {
    "objectID": "gles_panel.html",
    "href": "gles_panel.html",
    "title": "Case Study: Linking GLES Panel survey",
    "section": "",
    "text": "comming soon"
  },
  {
    "objectID": "gles_panel.html#the-gles-panel",
    "href": "gles_panel.html#the-gles-panel",
    "title": "Case Study: Linking GLES Panel survey",
    "section": "",
    "text": "comming soon"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Further resources",
    "section": "",
    "text": "comming soon"
  },
  {
    "objectID": "resources.html#helpful-eo-packages-in-r",
    "href": "resources.html#helpful-eo-packages-in-r",
    "title": "Further resources",
    "section": "",
    "text": "comming soon"
  },
  {
    "objectID": "datacubes.html",
    "href": "datacubes.html",
    "title": "Datacubes",
    "section": "",
    "text": "comming soon"
  },
  {
    "objectID": "datacubes.html#what-are-datacubes",
    "href": "datacubes.html#what-are-datacubes",
    "title": "Datacubes",
    "section": "",
    "text": "comming soon"
  },
  {
    "objectID": "quality_criteria.html#data-quality",
    "href": "quality_criteria.html#data-quality",
    "title": "EO data quality criteria",
    "section": "Data quality",
    "text": "Data quality\nData quality\nThe German Federal Statistical Office (DESTATIS) is currently evaluating the usage of remote sensing data for official statistical accounts in several projects as part of their experimental statistics.\n\nSmart business cycle statistics based on satellite data\nRemote sensing data and artifical intelligence in the register census\nSatellite-based early estimate of short-term economic development"
  },
  {
    "objectID": "quality_criteria.html#data-quality-assessment",
    "href": "quality_criteria.html#data-quality-assessment",
    "title": "EO data quality criteria",
    "section": "Data quality assessment",
    "text": "Data quality assessment\nEarth observation and remote sensing data have emerged as critical tools for applications ranging from environmental monitoring to the analysis of economic activities. However, the effective use of satellite imagery hinges on overcoming several data quality challenges. First, for many longitudinal research designs or official statistics, obtaining a continuous and reliable time series of images is essential. Yet, factors like persistent cloud cover, unfavorable shadow casting, and diffuse lighting conditions often lead to significant gaps in data. These gaps undermine the consistency and validity of temporal coverage.\nMoreover, the spatial resolution of satellite imagery plays a pivotal role in determining the utility of the data. While larger, easily identifiable objects like container ships can be detected at moderate resolutions (e.g., 10m per pixel), the identification of smaller, more closely packed objects such as cars and shipping containers demands super high resolution below 1m. The figure below, from the Destatis evaluation project of satellite data for business cycles, visualizes these distinctions quite intuitively. The Destatis evaluation project showed that free data from Sentinel-2 were only partly suitable for detection activities. The images were too coarse to be utilized for automated detection of objects like cars and containers.\n\n\n\nResolution of satellite images compared (Source: Destatis 2019).\n\n\nHigh-resolution requirements pose further challenges, as these images are typically provided by commercial satellite operators under a tasking model, which not only increases operational costs but also introduces delays in data availability and is normally limited to a small geographical area. Additionally, processing these high-resolution images requires considerable storage and computational resources.\nEven with free data sources, such as the Sentinel-fleet from the Copernicus programme, limitations in resolution mean that these images are not always suitable for applications requiring detailed object detection. “Snapshot” modes like from OCO-3 might generate high-resolution images which are also freely available but ultimately lack a consistent and global coverage. Consequently, the infrequent availability of high-quality images—exacerbated by weather constraints and the sporadic nature of commercial high-resolution tasking—remains one of the most significant hurdles in using satellite data to mirror dynamic phenomena like economic activity or population trends.\nAdvancements in satellite technology, particularly through the deployment of small, low-cost satellites, offer promising solutions. These new platforms are expected to provide higher temporal frequency and improved resolution at reduced costs, potentially mitigating many of the current data quality issues."
  },
  {
    "objectID": "issp.html#data-access-and-preparation",
    "href": "issp.html#data-access-and-preparation",
    "title": "Case Study: Linking ISSP survey data",
    "section": "Data access and preparation",
    "text": "Data access and preparation\nGiven that we want to aggregate the data on country-level, we first load country shapefiles, and download the data according to the spatial extent of the countries included in the survey. The ISSP has a diverse membership from North and South America, Europe, Africa, and Asia. Thus, we can work with a global spatial extent when downloading the EO indicator.\nWe need some packages to load and prepare the world map and process the raster files (rnaturalearth, sf, terra, and tidyverse). We also need the keyring-package to safely store our ECMWF-API key and the devtools-package to load the gxc-package.\n\n# Install and load required packages\nrequired_packages &lt;- c(\"keyring\", \"rnaturalearth\", \"sf\", \"tidyverse\", \"terra\", \"devtools\")\nnew_packages &lt;- required_packages[!(required_packages %in% installed.packages()[,\"Package\"])]\nif(length(new_packages)) install.packages(new_packages)\nlapply(required_packages, library, character.only = TRUE)\n\nLinking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.3.1; sf_use_s2() is TRUE\n\n\nterra 1.7.78\n\n\n\nAttache Paket: 'terra'\n\n\nDas folgende Objekt ist maskiert 'package:tidyr':\n\n    extract\n\n\nLade nötiges Paket: usethis\n\n\n\nAttache Paket: 'usethis'\n\n\nDas folgende Objekt ist maskiert 'package:sjlabelled':\n\n    tidy_labels\n\n\n[[1]]\n [1] \"keyring\"    \"lubridate\"  \"forcats\"    \"stringr\"    \"dplyr\"     \n [6] \"purrr\"      \"readr\"      \"tidyr\"      \"tibble\"     \"ggplot2\"   \n[11] \"tidyverse\"  \"sjlabelled\" \"haven\"      \"emo\"        \"stats\"     \n[16] \"graphics\"   \"grDevices\"  \"utils\"      \"datasets\"   \"methods\"   \n[21] \"base\"      \n\n[[2]]\n [1] \"rnaturalearth\" \"keyring\"       \"lubridate\"     \"forcats\"      \n [5] \"stringr\"       \"dplyr\"         \"purrr\"         \"readr\"        \n [9] \"tidyr\"         \"tibble\"        \"ggplot2\"       \"tidyverse\"    \n[13] \"sjlabelled\"    \"haven\"         \"emo\"           \"stats\"        \n[17] \"graphics\"      \"grDevices\"     \"utils\"         \"datasets\"     \n[21] \"methods\"       \"base\"         \n\n[[3]]\n [1] \"sf\"            \"rnaturalearth\" \"keyring\"       \"lubridate\"    \n [5] \"forcats\"       \"stringr\"       \"dplyr\"         \"purrr\"        \n [9] \"readr\"         \"tidyr\"         \"tibble\"        \"ggplot2\"      \n[13] \"tidyverse\"     \"sjlabelled\"    \"haven\"         \"emo\"          \n[17] \"stats\"         \"graphics\"      \"grDevices\"     \"utils\"        \n[21] \"datasets\"      \"methods\"       \"base\"         \n\n[[4]]\n [1] \"sf\"            \"rnaturalearth\" \"keyring\"       \"lubridate\"    \n [5] \"forcats\"       \"stringr\"       \"dplyr\"         \"purrr\"        \n [9] \"readr\"         \"tidyr\"         \"tibble\"        \"ggplot2\"      \n[13] \"tidyverse\"     \"sjlabelled\"    \"haven\"         \"emo\"          \n[17] \"stats\"         \"graphics\"      \"grDevices\"     \"utils\"        \n[21] \"datasets\"      \"methods\"       \"base\"         \n\n[[5]]\n [1] \"terra\"         \"sf\"            \"rnaturalearth\" \"keyring\"      \n [5] \"lubridate\"     \"forcats\"       \"stringr\"       \"dplyr\"        \n [9] \"purrr\"         \"readr\"         \"tidyr\"         \"tibble\"       \n[13] \"ggplot2\"       \"tidyverse\"     \"sjlabelled\"    \"haven\"        \n[17] \"emo\"           \"stats\"         \"graphics\"      \"grDevices\"    \n[21] \"utils\"         \"datasets\"      \"methods\"       \"base\"         \n\n[[6]]\n [1] \"devtools\"      \"usethis\"       \"terra\"         \"sf\"           \n [5] \"rnaturalearth\" \"keyring\"       \"lubridate\"     \"forcats\"      \n [9] \"stringr\"       \"dplyr\"         \"purrr\"         \"readr\"        \n[13] \"tidyr\"         \"tibble\"        \"ggplot2\"       \"tidyverse\"    \n[17] \"sjlabelled\"    \"haven\"         \"emo\"           \"stats\"        \n[21] \"graphics\"      \"grDevices\"     \"utils\"         \"datasets\"     \n[25] \"methods\"       \"base\"         \n\n\nWe load the shapefile containing country-level polygons and subset it to the most relevant variables.\n\n# Download world map data\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\nst_geometry(world)\n\nGeometry set for 242 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -89.99893 xmax: 180 ymax: 83.59961\nGeodetic CRS:  WGS 84\nFirst 5 geometries:\n\n\nMULTIPOLYGON (((31.28789 -22.40205, 31.19727 -2...\n\n\nMULTIPOLYGON (((30.39609 -15.64307, 30.25068 -1...\n\n\nMULTIPOLYGON (((53.08564 16.64839, 52.58145 16....\n\n\nMULTIPOLYGON (((104.064 10.39082, 104.083 10.34...\n\n\nMULTIPOLYGON (((-60.82119 9.138379, -60.94141 9...\n\n# Subset to relevant variables\nworld &lt;- world |&gt; \n  select(admin, iso_a3, geometry)\n\n# Plot world map\nplot(world[1])\n\n\n\n\n\n\n\n\nA final step before we can access the data from the Copernicus API is to store our API key. By setting it to “wf_api_key”, the function automatically retrieves the key.\n\n# Store as environment variable\n# Sys.setenv(WF_API_KEY = \"YOUR-API-KEY\")\n\napi_key &lt;- Sys.getenv(\"WF_API_KEY\")\n\nkeyring::key_set_with_value(service = \"wf_api_key\", password = api_key)\n\nNow we can access the data. We loop the download over the four years of the survey programme (1993, 2000, 2010, 2020) in order to create four separate files.\n\n# Year vector\nyears &lt;- c(\"1993\", \"2000\", \"2010\", \"2020\")\n\n# # API acess looped over four years\n# for (yr in years) {\n# \n#   # Create file names which include year\n#   file_name &lt;- paste0(\"era5_temperature\", yr, \".grib\")\n# \n#   # Specify API request\n#   request &lt;- list(\n#     data_format = \"grib\",\n#     variable = \"2m_temperature\",\n#     product_type = \"monthly_averaged_reanalysis\",\n#     time = \"00:00\",\n#     year = yr,\n#     month = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"),\n#     area = c(90, -180, -90, 180),\n#     dataset_short_name = \"reanalysis-era5-land-monthly-means\",\n#     target = file_name\n#   )\n# \n#   # Download data from C3S\n#   file_path &lt;- ecmwfr::wf_request(\n#     request = request,\n#     transfer = TRUE,\n#     path = \"./data/EO_data/C3S_data\",\n#     verbose = FALSE\n#   )\n# \n# }\n\n\ntemp_1993 &lt;- terra::rast(\"./data/EO_data/C3S_data/era5_temperature1993.grib\")\ntemp_2000 &lt;- terra::rast(\"./data/EO_data/C3S_data/era5_temperature2000.grib\")\ntemp_2010 &lt;- terra::rast(\"./data/EO_data/C3S_data/era5_temperature2010.grib\")\ntemp_2020 &lt;- terra::rast(\"./data/EO_data/C3S_data/era5_temperature2020.grib\")\n\nLet’s inspect the datacube for 2020 and plot the first layer of the 2020 datacube (January 2020). The attributes of the file tell us information on the dimensions (number of rows, columns, and layers), the resolution, spatial extent, the coordinate reference system, units, and time points.\n\ntemp_2020\n\nclass       : SpatRaster \ndimensions  : 1801, 3600, 12  (nrow, ncol, nlyr)\nresolution  : 0.1, 0.1  (x, y)\nextent      : -180.05, 179.95, -90.05, 90.05  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat Coordinate System imported from GRIB file \nsource      : era5_temperature2020.grib \nnames       : SFC (~e [C], SFC (~e [C], SFC (~e [C], SFC (~e [C], SFC (~e [C], SFC (~e [C], ... \nunit        :           C,           C,           C,           C,           C,           C, ... \ntime        : 2020-01-01 01:00:00 to 2020-12-01 01:00:00 UTC \n\nplot(temp_2020[[1]])\n\n\n\n\n\n\n\n\nNow we can aggregate the monthly values by year and country. We will check that our country polygons and the raster files have the same CRS and align, if necessary.\n\nfor (yr in years) {\n  temp_data &lt;- get(paste0(\"temp_\", yr))\n\n  # Check CRS of both datasets and adjust if necessary\n  if(!identical(crs(world), terra::crs(temp_data))) {\n    world &lt;- world |&gt;\n      st_transform(crs=st_crs(temp_data))\n  }\n\n  # Collapse the month layers into one layer by averaging across months\n  annual_values &lt;- terra::app(temp_data, fun = mean, na.rm = TRUE)\n\n  # Aggregate by country\n  country_values &lt;- terra::extract(\n      annual_values,\n      world,\n      fun = mean,\n      na.rm = TRUE\n    )\n\n  # Add values to shapefile\n  world[paste0(\"temp_\", yr)] &lt;- country_values[, 2]\n\n}\n\nprint(head(world))\n\nSimple feature collection with 6 features and 6 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.36621 ymin: -22.40205 xmax: 109.4449 ymax: 41.9062\nGeodetic CRS:  Coordinate System imported from GRIB file\n      admin iso_a3                       geometry temp_1993 temp_2000 temp_2010\n1  Zimbabwe    ZWE MULTIPOLYGON (((31.28789 -2...  294.3789  293.3119  294.4570\n2    Zambia    ZMB MULTIPOLYGON (((30.39609 -1...  294.8202  294.7319  295.1083\n3     Yemen    YEM MULTIPOLYGON (((53.08564 16...  297.5421  297.9850  298.3186\n4   Vietnam    VNM MULTIPOLYGON (((104.064 10....  295.9980  295.8958  296.7712\n5 Venezuela    VEN MULTIPOLYGON (((-60.82119 9...  297.7739  297.5062  298.3977\n6   Vatican    VAT MULTIPOLYGON (((12.43916 41...  288.3732  289.0528  288.5110\n  temp_2020\n1  294.6193\n2  295.2352\n3  298.2873\n4  296.7613\n5  298.7989\n6  289.6033\n\n\nNow that we have the focal values for all four survey years, we redo the process for the baseline period (1961-1990).\n\n# Year vector\nbaseline_years &lt;- as.character(1961:1970)\n\n# # Specify API request\n# request &lt;- list(\n#     data_format = \"grib\",\n#     variable = \"2m_temperature\",\n#     product_type = \"monthly_averaged_reanalysis\",\n#     time = \"00:00\",\n#     year = baseline_years,\n#     month = c(\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"),\n#     area = c(90, -180, -90, 180),\n#     dataset_short_name = \"reanalysis-era5-land-monthly-means\",\n#     target = \"era5_temperature1961-1990.grib\"\n#   )\n# \n# # Download data from C3S\n# file_path &lt;- ecmwfr::wf_request(\n#     request = request,\n#     transfer = TRUE,\n#     path = \"./data/EO_data/C3S_data\",\n#     verbose = FALSE\n#   )\n\n\ntemp_base &lt;- terra::rast(\"./data/EO_data/C3S_data/era5_temperature1961-1990.grib\")\n\n\n# Check CRS of both datasets and adjust if necessary\nif(!identical(crs(world), terra::crs(temp_base))) {\n  world &lt;- world |&gt;\n    st_transform(crs=st_crs(temp_base))\n  }\n\n# Collapse all into one layer by averaging across months and years\nannual_values &lt;- terra::app(temp_base, fun = mean, na.rm = TRUE)\n\n\n|---------|---------|---------|---------|\n=========================================\n                                          \n\n# Aggregate by country\ncountry_values &lt;- terra::extract(\n  annual_values,\n  world,\n  fun = mean,\n  na.rm = TRUE\n  )\n\n# Add values to shapefile\nworld$temp_base &lt;- country_values[, 2]\n\nprint(head(world))\n\nSimple feature collection with 6 features and 7 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -73.36621 ymin: -22.40205 xmax: 109.4449 ymax: 41.9062\nGeodetic CRS:  Coordinate System imported from GRIB file\n      admin iso_a3                       geometry temp_1993 temp_2000 temp_2010\n1  Zimbabwe    ZWE MULTIPOLYGON (((31.28789 -2...  294.3789  293.3119  294.4570\n2    Zambia    ZMB MULTIPOLYGON (((30.39609 -1...  294.8202  294.7319  295.1083\n3     Yemen    YEM MULTIPOLYGON (((53.08564 16...  297.5421  297.9850  298.3186\n4   Vietnam    VNM MULTIPOLYGON (((104.064 10....  295.9980  295.8958  296.7712\n5 Venezuela    VEN MULTIPOLYGON (((-60.82119 9...  297.7739  297.5062  298.3977\n6   Vatican    VAT MULTIPOLYGON (((12.43916 41...  288.3732  289.0528  288.5110\n  temp_2020 temp_base\n1  294.6193  294.1319\n2  295.2352  294.8179\n3  298.2873  297.3228\n4  296.7613  295.6958\n5  298.7989  297.8074\n6  289.6033  288.1982\n\n\nNow that we have the focal and baseline values, we calculate single deviations.\n\nworld &lt;- world |&gt;\n  dplyr::mutate(\n    diff_1993 = temp_1993 - temp_base,\n    diff_2000 = temp_2000 - temp_base,\n    diff_2010 = temp_2010 - temp_base,\n    diff_2020 = temp_2020 - temp_base\n  )\n\n# Plot 2020 deviation from baseline\nggplot(data = world) +\n  geom_sf(aes(fill = diff_2020)) +\n  scale_fill_viridis_c() +\n  theme_minimal() +\n  labs(\n    title = \"Absolute deviation between 2020 and baseline temperature\",\n    subtitle = \"Averaged across countries\",\n    fill = \"Temperature (K)\"\n  )"
  },
  {
    "objectID": "issp.html#spatial-linking-made-easy",
    "href": "issp.html#spatial-linking-made-easy",
    "title": "Case Study: Linking ISSP survey data",
    "section": "Spatial linking made easy",
    "text": "Spatial linking made easy\nThe manual approach described above is time- and code-intensive. Our gxc-package helps to automatize these steps. You can use the poly_link-function to directly link to every single observation in a dataset with the discussed EO indicator.\nWe need devtools to load the gxc-package.\n\n# # Load gxc package\n# if (!requireNamespace(\"gxc\", quietly = TRUE)) {\n#   devtools::install_github(\"denabel/gxc\")\n# }\n# \n# # Now load the package\n# library(gxc)\n\nWe exemplify the process for the 2020 wave. The function requires the last month of the focal time period as variable in the dataset. We add a date-variable to the dataset which records the last month of the focal period (December 2020).\n\n# Create fixed date-variable\n# world$date_raw &lt;- \"12-2020\"\n\nCheck out vignette for poly_link for detailed documentation.\n\n# ?gxc::poly_link\n\nSpecification of poly_link and data access.\n\n# dataset_out &lt;- gxc::poly_link(\n#   indicator = \"2m_temperature\",\n#   data = world,\n#   date_var = \"date_raw\",\n#   time_span = 11,\n#   time_lag = 0,\n#   baseline = c(\"1961\", \"1970\"),\n#   order = \"my\",\n#   path = \"./data/EO_data/C3S_data\",\n#   catalogue = \"reanalysis-era5-land-monthly-means\",\n#   by_hour = FALSE,\n#   keep_raw = FALSE)\n\n\n# print(head(world |&gt; \n#   filter(!is.na(mean_concern_2020)) |&gt; \n#   arrange(admin)))\n\nThis was a relatively easy example where we link data on the country-level. Data with a more fine-grained georeferencing or more complex temporal resolution requires even more flexible approaches. The gxc-package allows these custom-made linking approaches. The next example with the GLES Panel shows how to do it for observations with varying linking dates and small spatial buffer."
  },
  {
    "objectID": "issp.html#integrate-survey-and-eo-data",
    "href": "issp.html#integrate-survey-and-eo-data",
    "title": "Case Study: Linking ISSP survey data",
    "section": "Integrate survey and EO data",
    "text": "Integrate survey and EO data\nTurning to the survey data, we aggregate climate change concern across country-waves and link it with the shapefile.\n\nmean_concern &lt;- issp |&gt;\n  group_by(country_new, year) |&gt;\n  summarize(mean_concern = mean(concern, na.rm=TRUE),\n            se_concern = sd(concern, na.rm=TRUE) / sqrt(n()))\n\n`summarise()` has grouped output by 'country_new'. You can override using the\n`.groups` argument.\n\nggplot(mean_concern, aes(x = year, y = mean_concern, color = country_new, group = country_new)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = mean_concern - se_concern, ymax = mean_concern + se_concern), width = .5) +\n  geom_line()+\n  labs(title = \"Mean climate change concern across countries\",\n       x = \"Survey Wave\", y = \"Mean climate change concern\", color = \"Country\") +\n  facet_wrap(~country_new, ncol=5)+\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\nmean_concern_wide &lt;- mean_concern |&gt;\n  select(!se_concern) |&gt;\n  pivot_wider(\n    names_from = year,\n    values_from = mean_concern,\n    names_glue = \"{.value}_{year}\",\n    names_sort = TRUE)\n\nworld &lt;- left_join(world, mean_concern_wide, by = c(\"admin\" = \"country_new\"))\n\nprint(head(world |&gt; \n  filter(!is.na(mean_concern_2020)) |&gt; \n  arrange(admin)))\n\nSimple feature collection with 6 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -61.79409 ymin: -54.74922 xmax: 158.9589 ymax: 70.06484\nGeodetic CRS:  Coordinate System imported from GRIB file\n      admin iso_a3 temp_1993 temp_2000 temp_2010 temp_2020 temp_base  diff_1993\n1 Australia    AUS  294.8993  294.2218  294.7285  295.7058  295.0203 -0.1210429\n2   Austria    AUT  278.7910  279.8814  278.3763  280.4286  278.1573  0.6337671\n3   Croatia    HRV  283.9717  285.6562  283.9234  285.5485  283.5202  0.4515663\n4   Denmark    DNK  280.9850  282.4755  280.1209  283.3281  280.6341  0.3508730\n5   Finland    FIN  274.8894  276.5970  274.3150  277.6573  274.4078  0.4816045\n6    France    -99  285.0149  285.6563  284.6554  286.9104  284.6791  0.3357822\n   diff_2000   diff_2010 diff_2020 mean_concern_1993 mean_concern_2000\n1 -0.7985866 -0.29184766 0.6854605           3.73951                NA\n2  1.7241737  0.21901826 2.2712879                NA          3.996714\n3  2.1360691  0.40327552 2.0283470                NA                NA\n4  1.8414213 -0.51319651 2.6940181                NA          3.402542\n5  2.1891151 -0.09287108 3.2494134                NA          3.302792\n6  0.9772204 -0.02364492 2.2312917                NA                NA\n  mean_concern_2010 mean_concern_2020                       geometry\n1          3.306898          3.833178 MULTIPOLYGON (((143.1789 -1...\n2          3.804325          4.182692 MULTIPOLYGON (((9.527539 47...\n3          4.010915          3.990964 MULTIPOLYGON (((13.57793 45...\n4          3.498276          3.642857 MULTIPOLYGON (((12.56875 55...\n5          3.511832          3.924270 MULTIPOLYGON (((24.15547 65...\n6          3.474752          3.832879 MULTIPOLYGON (((9.480371 42...\n\n\nThe data is ready to use for any further analysis."
  },
  {
    "objectID": "quality_criteria.html#data-quality-criteria",
    "href": "quality_criteria.html#data-quality-criteria",
    "title": "EO data quality criteria",
    "section": "",
    "text": "Earth observation data often comes in raster-format, sometimes in vector-format. In order to assess the quality of a specific EO product and determine wether it is suitable for our specific research purpose, we need to be aware of a few terminologies. These are explained below. For researchers who regularly work with geographical data, these should sound familiar.\n\n\n\nCreating high quality maps (AI generated image)."
  },
  {
    "objectID": "performance.html",
    "href": "performance.html",
    "title": "Performance",
    "section": "",
    "text": "Here, you can explore performance of the gxc-package in terms of execution time. We have tested several specifications and varied the most important parameters which might affect the processing time and which are representative of typical specifications in previous studies:\n\nSample size: 10-5000\nSpatial extent: City, national, continental, worldwide\nFocal period: Averages for month, season, and year\nBaseline period: No baseline, 10y baseline, 30y baseline.\n\nWe are accessing the 2m_temperature indicator from the ERA5-Land dataset for this exercise. The code was run on an average office laptop (Intel Core i7-10510U CPU, 16GB RAM, Windows 10). Below, you find the detailed code for the sample generation and execution."
  },
  {
    "objectID": "performance.html#simulating-linking-data-of-different-sizes",
    "href": "performance.html#simulating-linking-data-of-different-sizes",
    "title": "Performance",
    "section": "",
    "text": "`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n# Load packages\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(rnaturalearth)\nlibrary(gridExtra)\nlibrary(keyring)\nlibrary(tictoc)\nlibrary(gxc)\n\n\n# Load shapefiles and prepare grids ---------------------------------------\n\n# Retrieve Germany's bounding box\ngermany &lt;- ne_countries(country = \"Germany\", \n                        scale = \"medium\", \n                        returnclass = \"sf\") \n  \n# Generate the grid\ngrid_national &lt;- sf::st_make_grid(\n  st_as_sfc(st_bbox(germany)), \n  n = c(100,100)\n  ) \n  \ngrid_national &lt;- st_sf(geometry = grid_national)\n\n# Assign WGS84 CRS\nst_crs(grid_national) &lt;- 4326\n\n# Define the four corner cells (by relying on the indexing of the grid cell)\nll_index &lt;- 1\nlr_index &lt;- 100\nul_index &lt;- 9901\nur_index &lt;- 10000\n\ncorner_indices &lt;- c(ll_index, lr_index, ul_index, ur_index)\n\n\n# Generate sample grids ---------------------------------------------------\n\n# Sample-function (always including the four corner cells).\nsample_grid &lt;- function(n, my_grid, extent_polygons) {\n  if(n &lt; 4) stop(\"n must be at least 4 to include all corner cells.\")\n  \n  # Calculate n-corner polygons\n  n_random &lt;- n - length(extent_polygons)\n  \n  # Exclude the corner cells from the random sample\n  remaining &lt;- my_grid[-extent_polygons, ]\n  \n  # Randomly sample from the remaining cells\n  random_sample &lt;- remaining[sample(nrow(remaining), n_random), ]\n  \n  # Combine the corner cells with the random sample\n  sampled &lt;- rbind(my_grid[extent_polygons, ], random_sample)\n  return(sampled)\n}\n\n# Sample grids with various n\nsample_sizes &lt;- c(10, 20, 50, 100, 200, 500, 1000, 2000, 5000)\n\nsamples_list &lt;- list()\n\n# Loop over the sample sizes\nfor (i in seq_along(sample_sizes)) {\n  n &lt;- sample_sizes[i]\n\n  samples_list[[i]] &lt;- sample_grid(n, grid_national, corner_indices)\n}\n\nnames(samples_list) &lt;- paste0(\"sample_\", sample_sizes)\n\n\n# Visualize\nplots &lt;- lapply(seq_along(sample_sizes), function(i) {\n  n &lt;- sample_sizes[i]\n  sampled_sf &lt;- samples_list[[i]]\n  \n  ggplot() +\n    geom_sf(data = sampled_sf, \n            fill = \"#c994c7\", \n            color = \"#dd1c77\", \n            size = 0.3) +\n    geom_sf(data = germany, fill = NA, color = \"#2c7fb8\", size = 0.8) +\n    ggtitle(paste(\"Sample size:\", n)) +\n    theme_minimal()\n})\n\ngridExtra::grid.arrange(grobs = plots[-c(1:3)], ncol = 3)\n\n\n\n\n\n\n\n\n\n# # Loop over poly_link-function and measure elapsed time -------------------\n# \n# # Add random time identifier in grids\n# samples_list &lt;- lapply(samples_list, function(grid) {\n#   grid$date_raw &lt;- \"12-2020\"\n#   grid\n# })\n# \n# # Set up API access\n# api_key &lt;- Sys.getenv(\"WF_API_KEY\")\n# \n# keyring::key_set_with_value(service = \"wf_api_key\", password = api_key)\n# \n# # Set up new dataframe measuring elapsed time\n# results_df &lt;- data.frame(\n#   spatial_extent = character(),\n#   sample_size = integer(),\n#   focal_period = character(),\n#   seconds = numeric(),\n#   stringsAsFactors = FALSE\n# )\n# \n# # Loop over the list of samples using tictoc to measure execution time.\n# time_span_values &lt;- c(0,2,11)\n# \n# for (i in seq_along(samples_list)) {\n#   sample_size &lt;- sample_sizes[i]\n#   sample_polygons &lt;- samples_list[[i]]\n#   \n#   for (ts in time_span_values) {\n#   \n#   # Start timing using tic().\n#   tic()  \n#   # Run poly_link function\n#   poly_result &lt;- poly_link(\n#     indicator = \"2m_temperature\",\n#     data = sample_polygons,\n#     date_var = \"date_raw\",\n#     time_span = ts,\n#     time_lag = 0,\n#     baseline = FALSE,\n#     order = \"my\",\n#     path = \"./data\",\n#     catalogue = \"reanalysis-era5-land-monthly-means\",\n#     by_hour = FALSE,\n#     keep_raw = FALSE)\n#   # Stop timing and capture the elapsed time.\n#   toc_out &lt;- toc(quiet = TRUE)\n#   \n#   # The output toc_out is a list with the start and stop times. Compute the elapsed time.\n#   elapsed_time &lt;- toc_out$toc - toc_out$tic\n#   \n#   focal_period &lt;- if (ts == 0) {\n#     \"Month\"\n#   } else if (ts == 2) {\n#     \"Season\"\n#   } else if (ts == 11) {\n#     \"Year\"\n#   } else {\n#     NA\n#   }\n#   \n#   # Append the results to the results_df data frame.\n#   results_df &lt;- rbind(results_df, data.frame(\n#     spatial_extent = \"Germany\",\n#     sample_size = sample_size,\n#     focal_period = focal_period,\n#     seconds = elapsed_time,\n#     stringsAsFactors = FALSE\n#   ))\n#   \n#   # Save as rds\n#   saveRDS(results_df, file = \"./data/performance_results.rds\")\n#   }\n# }\n\n\nresults_df &lt;- readRDS(\"./data/performance_results.rds\")\n\n# In case you have multiple extents in the future, we split the data:\ndf_list &lt;- results_df %&gt;% \n  group_by(spatial_extent) %&gt;% \n  group_split()\n\n# Create a base ggplot without data; then add a geom for each extent.\np &lt;- ggplot() +\n  labs(\n    title = \"Execution Time vs Sample Size\",\n    x = \"Sample Size\",\n    y = \"Execution Time (minutes)\",\n    color = \"Focal Period\"\n  ) +\n  scale_color_manual(values = c(\"month\" = \"#dd1c77\",\n                                \"seasonal\" = \"#225ea8\",\n                                \"yearly\" = \"#7fcdbb\"))+\n  theme_minimal()\n\n# Loop through the list of data subsets and add a layer for each spatial extent.\nfor (d in df_list) {\n  p &lt;- p +\n    geom_point(data = d, aes(x = sample_size, y = seconds / 60, color = focal_period)) +\n    geom_smooth(data = d, aes(x = sample_size, y = seconds / 60, color = focal_period),\n                method = \"loess\", se = FALSE)\n}\n\n# Convert to plotly object\np_plotly &lt;- ggplotly(p)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n# Now add an update menu that will filter traces based on spatial_extent.\n# (In the converted ggplotly object, traces will be in the order that the layers were added.\n#  You may need to adjust the \"visible\" vectors manually once you have more than one spatial extent.)\n# For this example, we assume that the first set of traces (say, traces 1 & 2) belong to \"Germany\"\n# and later on additional traces would correspond to other extents.\n\n# Define update menu for spatial_extent (first drop-down)\nspatial_extent_buttons &lt;- list(\n  list(\n    method = \"restyle\",\n    args = list(\"visible\", list(TRUE, TRUE)), # Adjust based on how many traces belong to Germany\n    label = \"Germany\"\n  ),\n  list(\n    method = \"restyle\",\n    args = list(\"visible\", list(FALSE, FALSE)), # Placeholder for a second category\n    label = \"Europe\"\n  )\n)\n\n# Define a second update menu for another variable (e.g., line type or another aesthetic).\n# In this dummy example, we'll pretend to change the line dash.\nbaseline_buttons &lt;- list(\n  list(\n    method = \"restyle\",\n    args = list(\"line.dash\", \"solid\"),\n    label = \"No baseline\"\n  ),\n  list(\n    method = \"restyle\",\n    args = list(\"line.dash\", \"dot\"),\n    label = \"10y baseline\"\n  ),\n  list(\n    method = \"restyle\",\n    args = list(\"line.dash\", \"dash\"),\n    label = \"30y baseline\"\n  )\n)\n\n# Add both update menus to the layout.\np_plotly &lt;- p_plotly %&gt;% layout(\n  # Adjust the title position (y value less than 1 moves it downward)\n  title = list(\n    text = \"Execution Time vs Sample Size\",\n    y = 0.85  # Lower the title; experiment with this value\n  ),\n  # Increase the top margin to give more room for the buttons above the title\n  margin = list(t = 150),\n  updatemenus = list(\n    # First drop-down menu: spatial_extent\n    list(\n      type = \"dropdown\",\n      active = 0,\n      buttons = spatial_extent_buttons,\n      x = 0, y = 1.12,  # adjust position as needed\n      xanchor = \"left\",\n      yanchor = \"top\"\n    ),\n    # Second drop-down menu: line dash style\n    list(\n      type = \"dropdown\",\n      active = 0,\n      buttons = baseline_buttons,\n      x = 0.2, y = 1.12,  # adjust position as needed\n      xanchor = \"left\",\n      yanchor = \"top\"\n    )\n  ),\n  annotations = list(\n    list(\n      x = 0, y = 1.22,\n      xref = \"paper\", yref = \"paper\",\n      text = \"Spatial extent\",\n      showarrow = FALSE,\n      font = list(size = 14)\n    ),\n    list(\n      x = 0.2, y = 1.22,\n      xref = \"paper\", yref = \"paper\",\n      text = \"Baseline period\",\n      showarrow = FALSE,\n      font = list(size = 14)\n    )\n  )\n)\n\n# Display the interactive plot.\np_plotly"
  },
  {
    "objectID": "performance.html#how-long-will-the-function-run-on-an-average-laptop",
    "href": "performance.html#how-long-will-the-function-run-on-an-average-laptop",
    "title": "Performance",
    "section": "",
    "text": "Here, you can explore performance of the gxc-package in terms of execution time. We have tested several specifications and varied the most important parameters which might affect the processing time and which are representative of typical specifications in existing studies:\n\nSample size: 10-5000\nSpatial extent: City, national, continental, worldwide\nFocal period: Averages for month, season, and year\nBaseline period: No baseline, 10y baseline, 30y baseline.\n\nWe are accessing the 2m_temperature indicator from the ERA5-Land dataset for this exercise. The code was run on an average office laptop (Intel Core i7-10510U CPU, 16GB RAM, Windows 10). Below, you find the detailed code for the sample generation and execution."
  },
  {
    "objectID": "performance.html#replication-code",
    "href": "performance.html#replication-code",
    "title": "Performance",
    "section": "Replication code",
    "text": "Replication code\n\n# Load packages\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(plotly)\nlibrary(rnaturalearth)\nlibrary(gridExtra)\nlibrary(keyring)\nlibrary(tictoc)\nlibrary(gxc)"
  },
  {
    "objectID": "performance.html#generate-random-sample",
    "href": "performance.html#generate-random-sample",
    "title": "Performance",
    "section": "Generate random sample",
    "text": "Generate random sample\n\n# Load shapefiles and prepare grids ---------------------------------------\n\n# Retrieve Germany's bounding box\ngermany &lt;- ne_countries(country = \"Germany\", \n                        scale = \"medium\", \n                        returnclass = \"sf\") \n  \n# Generate the grid\ngrid_national &lt;- sf::st_make_grid(\n  st_as_sfc(st_bbox(germany)), \n  n = c(100,100)\n  ) \n  \ngrid_national &lt;- st_sf(geometry = grid_national)\n\n# Assign WGS84 CRS\nst_crs(grid_national) &lt;- 4326\n\n# Define the four corner cells (by relying on the indexing of the grid cell)\nll_index &lt;- 1\nlr_index &lt;- 100\nul_index &lt;- 9901\nur_index &lt;- 10000\n\ncorner_indices &lt;- c(ll_index, lr_index, ul_index, ur_index)\n\n\n# Generate sample grids ---------------------------------------------------\n\n# Sample-function (always including the four corner cells).\nsample_grid &lt;- function(n, my_grid, extent_polygons) {\n  if(n &lt; 4) stop(\"n must be at least 4 to include all corner cells.\")\n  \n  # Calculate n-corner polygons\n  n_random &lt;- n - length(extent_polygons)\n  \n  # Exclude the corner cells from the random sample\n  remaining &lt;- my_grid[-extent_polygons, ]\n  \n  # Randomly sample from the remaining cells\n  random_sample &lt;- remaining[sample(nrow(remaining), n_random), ]\n  \n  # Combine the corner cells with the random sample\n  sampled &lt;- rbind(my_grid[extent_polygons, ], random_sample)\n  return(sampled)\n}\n\n# Sample grids with various n\nsample_sizes &lt;- c(10, 20, 50, 100, 200, 500, 1000, 2000, 5000)\n\nsamples_list &lt;- list()\n\n# Loop over the sample sizes\nfor (i in seq_along(sample_sizes)) {\n  n &lt;- sample_sizes[i]\n\n  samples_list[[i]] &lt;- sample_grid(n, grid_national, corner_indices)\n}\n\nnames(samples_list) &lt;- paste0(\"sample_\", sample_sizes)\n\n\n# Visualize\nplots &lt;- lapply(seq_along(sample_sizes), function(i) {\n  n &lt;- sample_sizes[i]\n  sampled_sf &lt;- samples_list[[i]]\n  \n  ggplot() +\n    geom_sf(data = sampled_sf, \n            fill = \"#c994c7\", \n            color = \"#dd1c77\", \n            size = 0.3) +\n    geom_sf(data = germany, fill = NA, color = \"#2c7fb8\", size = 0.8) +\n    ggtitle(paste(\"Sample size:\", n)) +\n    theme_minimal()\n})\n\ngridExtra::grid.arrange(grobs = plots[-c(1:3)], ncol = 3)"
  },
  {
    "objectID": "performance.html#calculate-execution-time-for-each-specification",
    "href": "performance.html#calculate-execution-time-for-each-specification",
    "title": "Performance",
    "section": "Calculate execution time for each specification",
    "text": "Calculate execution time for each specification\n\n# # Loop over poly_link-function and measure elapsed time -------------------\n# \n# # Add random time identifier in grids\n# samples_list &lt;- lapply(samples_list, function(grid) {\n#   grid$date_raw &lt;- \"12-2020\"\n#   grid\n# })\n# \n# # Set up API access\n# api_key &lt;- Sys.getenv(\"WF_API_KEY\")\n# \n# keyring::key_set_with_value(service = \"wf_api_key\", password = api_key)\n# \n# # Set up new dataframe measuring elapsed time\n# results_df &lt;- data.frame(\n#   spatial_extent = character(),\n#   sample_size = integer(),\n#   focal_period = character(),\n#   seconds = numeric(),\n#   stringsAsFactors = FALSE\n# )\n# \n# # Loop over the list of samples using tictoc to measure execution time.\n# time_span_values &lt;- c(0,2,11)\n# \n# for (i in seq_along(samples_list)) {\n#   sample_size &lt;- sample_sizes[i]\n#   sample_polygons &lt;- samples_list[[i]]\n#   \n#   for (ts in time_span_values) {\n#   \n#   # Start timing using tic().\n#   tic()  \n#   # Run poly_link function\n#   poly_result &lt;- poly_link(\n#     indicator = \"2m_temperature\",\n#     data = sample_polygons,\n#     date_var = \"date_raw\",\n#     time_span = ts,\n#     time_lag = 0,\n#     baseline = FALSE,\n#     order = \"my\",\n#     path = \"./data\",\n#     catalogue = \"reanalysis-era5-land-monthly-means\",\n#     by_hour = FALSE,\n#     keep_raw = FALSE)\n#   # Stop timing and capture the elapsed time.\n#   toc_out &lt;- toc(quiet = TRUE)\n#   \n#   # The output toc_out is a list with the start and stop times. Compute the elapsed time.\n#   elapsed_time &lt;- toc_out$toc - toc_out$tic\n#   \n#   focal_period &lt;- if (ts == 0) {\n#     \"Month\"\n#   } else if (ts == 2) {\n#     \"Season\"\n#   } else if (ts == 11) {\n#     \"Year\"\n#   } else {\n#     NA\n#   }\n#   \n#   # Append the results to the results_df data frame.\n#   results_df &lt;- rbind(results_df, data.frame(\n#     spatial_extent = \"Germany\",\n#     sample_size = sample_size,\n#     focal_period = focal_period,\n#     seconds = elapsed_time,\n#     stringsAsFactors = FALSE\n#   ))\n#   \n#   # Save as rds\n#   saveRDS(results_df, file = \"./data/performance_results.rds\")\n#   }\n# }"
  },
  {
    "objectID": "performance.html#visualize",
    "href": "performance.html#visualize",
    "title": "Performance",
    "section": "Visualize",
    "text": "Visualize\n\nresults_df &lt;- readRDS(\"./data/performance_results.rds\")\n\n# split data based on groups for tabs\ndf_list &lt;- results_df %&gt;% \n  group_by(spatial_extent) %&gt;% \n  group_split()\n\n# Default ggplot\np &lt;- ggplot() +\n  labs(\n    title = \"Execution Time vs Sample Size\",\n    x = \"Sample Size\",\n    y = \"Execution Time (minutes)\",\n    color = \"Focal Period\"\n  ) +\n  scale_color_manual(values = c(\"month\" = \"#dd1c77\",\n                                \"seasonal\" = \"#225ea8\",\n                                \"yearly\" = \"#7fcdbb\"))+\n  theme_minimal()\n\n# Looping through list of subsets\nfor (d in df_list) {\n  p &lt;- p +\n    geom_point(data = d, aes(x = sample_size, y = seconds / 60, color = focal_period)) +\n    geom_smooth(data = d, aes(x = sample_size, y = seconds / 60, color = focal_period),\n                method = \"loess\", se = FALSE)\n}\n\n# Convert to plotly object\np_plotly &lt;- ggplotly(p)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n# Update layout\n\n# Create buttons\n# Spatial extent\nspatial_extent_buttons &lt;- list(\n  list(\n    method = \"restyle\",\n    args = list(\"visible\", list(TRUE, TRUE)),\n    label = \"Germany\"\n  ),\n  list(\n    method = \"restyle\",\n    args = list(\"visible\", list(FALSE, FALSE)),\n    label = \"Europe\"\n  )\n)\n\n# Baseline period\nbaseline_buttons &lt;- list(\n  list(\n    method = \"restyle\",\n    args = list(\"line.dash\", \"solid\"),\n    label = \"No baseline\"\n  ),\n  list(\n    method = \"restyle\",\n    args = list(\"line.dash\", \"dot\"),\n    label = \"10y baseline\"\n  ),\n  list(\n    method = \"restyle\",\n    args = list(\"line.dash\", \"dash\"),\n    label = \"30y baseline\"\n  )\n)\n\n# Adjust layout accordingly\np_plotly &lt;- p_plotly %&gt;% layout(\n  title = list(\n    text = \"Execution Time vs Sample Size\",\n    y = 0.85\n  ),\n  # Increase the top margin to give more space\n  margin = list(t = 150),\n  updatemenus = list(\n    list(\n      type = \"dropdown\",\n      active = 0,\n      buttons = spatial_extent_buttons,\n      x = 0, y = 1.12,\n      xanchor = \"left\",\n      yanchor = \"top\"\n    ),\n    list(\n      type = \"dropdown\",\n      active = 0,\n      buttons = baseline_buttons,\n      x = 0.2, y = 1.12,\n      xanchor = \"left\",\n      yanchor = \"top\"\n    )\n  ),\n  annotations = list(\n    list(\n      x = 0, y = 1.22,\n      xref = \"paper\", yref = \"paper\",\n      text = \"Spatial extent\",\n      showarrow = FALSE,\n      font = list(size = 14)\n    ),\n    list(\n      x = 0.2, y = 1.22,\n      xref = \"paper\", yref = \"paper\",\n      text = \"Baseline period\",\n      showarrow = FALSE,\n      font = list(size = 14)\n    )\n  )\n)\n\n# Display the interactive plot.\n# p_plotly"
  },
  {
    "objectID": "performance.html#how-long-will-the-package-run-on-an-average-laptop",
    "href": "performance.html#how-long-will-the-package-run-on-an-average-laptop",
    "title": "Performance",
    "section": "",
    "text": "Here, you can explore performance of the gxc-package in terms of execution time. We have tested several specifications and varied the most important parameters which might affect the processing time and which are representative of typical specifications in previous studies:\n\nSample size: 10-5000\nSpatial extent: City, national, continental, worldwide\nFocal period: Averages for month, season, and year\nBaseline period: No baseline, 10y baseline, 30y baseline.\n\nWe are accessing the 2m_temperature indicator from the ERA5-Land dataset for this exercise. The code was run on an average office laptop (Intel Core i7-10510U CPU, 16GB RAM, Windows 10). Below, you find the detailed code for the sample generation and execution."
  }
]